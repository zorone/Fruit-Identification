{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDA8AK7QOq-"
      },
      "source": [
        "## Setup Python Environment \n",
        "\n",
        "The next cell sets up the dependencies in required for the notebook, run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y2gs-PL4xDkZ",
        "outputId": "e9f328de-d6a4-479e-adf1-bd85e36573e0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'apt-get' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.0)\n",
            "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.26.4)\n",
            "Requirement already satisfied: matplotlib in c:\\programdata\\anaconda3\\lib\\site-packages (3.8.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
            "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\bitarray-2.9.2-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\bitstring-4.2.0b2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\configparser-7.0.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\docopt-0.6.2-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\ecdsa-0.18.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\edlclient-3.62-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\esptool-4.7.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\exscript-2.6.28-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\intelhex-2.3.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\passlib-1.7.4-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\pycryptodome-3.20.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\pycryptodomex-3.20.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\pylzma-0.5.0-py3.11-win-amd64.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\pyserial-3.5-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\pyusb-1.2.1-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\reedsolo-1.7.0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "DEPRECATION: Loading egg at c:\\programdata\\anaconda3\\lib\\site-packages\\usb-0.0.83.dev0-py3.11.egg is deprecated. pip 24.3 will enforce this behaviour change. A possible replacement is to use pip for package installation.. Discussion can be found at https://github.com/pypa/pip/issues/12330\n",
            "'tensorflow_version' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "^C\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Setup environment\n",
        "!apt-get -qq install xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lwkeshJk7dg"
      },
      "source": [
        "# Upload Data\n",
        "\n",
        "1. Open the panel on the left side of Colab by clicking on the __>__\n",
        "1. Select the Files tab\n",
        "1. Drag `csv` files from your computer to the tab to upload them into colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSxUeYPNQbOg"
      },
      "source": [
        "# Train Neural Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk414PU3oy3"
      },
      "source": [
        "## Parse and prepare the data\n",
        "\n",
        "The next cell parses the csv files and transforms them to a format that will be used to train the full connected neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "AGChd1FAk5_j",
        "outputId": "2c638e84-01b5-44ca-b0be-970bf23863dc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
            "  from pandas.core import (\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'tensorflow'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfileinput\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import fileinput\n",
        "\n",
        "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
        "\n",
        "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
        "# the same random numbers each time the notebook is run\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "CLASSES = [];\n",
        "\n",
        "for file in os.listdir(\"/content/\"):\n",
        "    if file.endswith(\".csv\"):\n",
        "        CLASSES.append(os.path.splitext(file)[0])\n",
        "\n",
        "CLASSES.sort()\n",
        "\n",
        "SAMPLES_WINDOW_LEN = 1\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_CLASSES = np.eye(NUM_CLASSES)\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for class_index in range(NUM_CLASSES):\n",
        "  objectClass = CLASSES[class_index]\n",
        "  df = pd.read_csv(\"/content/\" + objectClass + \".csv\")\n",
        "  columns = list(df)\n",
        "  # get rid of pesky empty value lines of csv which cause NaN inputs to TensorFlow\n",
        "  df = df.dropna()\n",
        "  df = df.reset_index(drop=True)\n",
        "   \n",
        "  # calculate the number of objectClass recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_WINDOW_LEN)\n",
        "  print(f\"\\u001b[32;4m{objectClass}\\u001b[0m class will be output \\u001b[32m{class_index}\\u001b[0m of the classifier\")\n",
        "  print(f\"{num_recordings} samples captured for training with inputs {list(df)} \\n\")\n",
        "\n",
        "  # graphing\n",
        "  plt.rcParams[\"figure.figsize\"] = (10,1)\n",
        "  pixels = np.array([df['Red'],df['Green'],df['Blue']],float)\n",
        "  pixels = np.transpose(pixels)\n",
        "  for i in range(num_recordings):\n",
        "    plt.axvline(x=i, linewidth=8, color=tuple(pixels[i]/np.max(pixels[i], axis=0)))\n",
        "  plt.show()\n",
        "  \n",
        "  #tensors\n",
        "  output = ONE_HOT_ENCODED_CLASSES[class_index]\n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    row = []\n",
        "    for c in columns:\n",
        "      row.append(df[c][i])\n",
        "    tensor += row\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")\n",
        "\n",
        "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
        "# https://stackoverflow.com/a/37710486/2020087\n",
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8qlSAX1b6Yv"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGNFa-lX24Qo",
        "outputId": "8c773d2d-9b5c-4b84-e6b5-d56c4d13e347"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/400\n",
            "105/105 [==============================] - 2s 7ms/step - loss: 0.2143 - mae: 0.4349 - val_loss: 0.2049 - val_mae: 0.4235\n",
            "Epoch 2/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.2003 - mae: 0.4154 - val_loss: 0.1953 - val_mae: 0.4084\n",
            "Epoch 3/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1931 - mae: 0.4034 - val_loss: 0.1885 - val_mae: 0.3987\n",
            "Epoch 4/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1861 - mae: 0.3953 - val_loss: 0.1810 - val_mae: 0.3842\n",
            "Epoch 5/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1789 - mae: 0.3835 - val_loss: 0.1714 - val_mae: 0.3746\n",
            "Epoch 6/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1698 - mae: 0.3734 - val_loss: 0.1613 - val_mae: 0.3614\n",
            "Epoch 7/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1599 - mae: 0.3601 - val_loss: 0.1503 - val_mae: 0.3474\n",
            "Epoch 8/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1482 - mae: 0.3462 - val_loss: 0.1380 - val_mae: 0.3289\n",
            "Epoch 9/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1357 - mae: 0.3278 - val_loss: 0.1252 - val_mae: 0.3166\n",
            "Epoch 10/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1221 - mae: 0.3105 - val_loss: 0.1119 - val_mae: 0.2915\n",
            "Epoch 11/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.1080 - mae: 0.2887 - val_loss: 0.0969 - val_mae: 0.2754\n",
            "Epoch 12/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0930 - mae: 0.2667 - val_loss: 0.0834 - val_mae: 0.2544\n",
            "Epoch 13/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0771 - mae: 0.2407 - val_loss: 0.0676 - val_mae: 0.2251\n",
            "Epoch 14/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0624 - mae: 0.2145 - val_loss: 0.0553 - val_mae: 0.2034\n",
            "Epoch 15/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0493 - mae: 0.1888 - val_loss: 0.0427 - val_mae: 0.1725\n",
            "Epoch 16/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0373 - mae: 0.1612 - val_loss: 0.0319 - val_mae: 0.1494\n",
            "Epoch 17/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0278 - mae: 0.1367 - val_loss: 0.0244 - val_mae: 0.1251\n",
            "Epoch 18/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0207 - mae: 0.1152 - val_loss: 0.0183 - val_mae: 0.1054\n",
            "Epoch 19/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0158 - mae: 0.0974 - val_loss: 0.0140 - val_mae: 0.0899\n",
            "Epoch 20/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0119 - mae: 0.0817 - val_loss: 0.0105 - val_mae: 0.0759\n",
            "Epoch 21/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0092 - mae: 0.0687 - val_loss: 0.0090 - val_mae: 0.0656\n",
            "Epoch 22/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0073 - mae: 0.0583 - val_loss: 0.0067 - val_mae: 0.0550\n",
            "Epoch 23/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0061 - mae: 0.0504 - val_loss: 0.0056 - val_mae: 0.0481\n",
            "Epoch 24/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0050 - mae: 0.0434 - val_loss: 0.0050 - val_mae: 0.0435\n",
            "Epoch 25/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0043 - mae: 0.0385 - val_loss: 0.0042 - val_mae: 0.0388\n",
            "Epoch 26/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0039 - mae: 0.0340 - val_loss: 0.0037 - val_mae: 0.0350\n",
            "Epoch 27/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0034 - mae: 0.0306 - val_loss: 0.0033 - val_mae: 0.0317\n",
            "Epoch 28/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0031 - mae: 0.0276 - val_loss: 0.0028 - val_mae: 0.0280\n",
            "Epoch 29/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0028 - mae: 0.0250 - val_loss: 0.0024 - val_mae: 0.0253\n",
            "Epoch 30/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0027 - mae: 0.0230 - val_loss: 0.0023 - val_mae: 0.0243\n",
            "Epoch 31/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0025 - mae: 0.0214 - val_loss: 0.0021 - val_mae: 0.0226\n",
            "Epoch 32/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0200 - val_loss: 0.0019 - val_mae: 0.0212\n",
            "Epoch 33/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0022 - mae: 0.0188 - val_loss: 0.0018 - val_mae: 0.0195\n",
            "Epoch 34/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0023 - mae: 0.0172 - val_loss: 0.0016 - val_mae: 0.0185\n",
            "Epoch 35/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0021 - mae: 0.0163 - val_loss: 0.0017 - val_mae: 0.0193\n",
            "Epoch 36/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0020 - mae: 0.0159 - val_loss: 0.0015 - val_mae: 0.0173\n",
            "Epoch 37/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0150 - val_loss: 0.0014 - val_mae: 0.0158\n",
            "Epoch 38/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0139 - val_loss: 0.0013 - val_mae: 0.0144\n",
            "Epoch 39/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0019 - mae: 0.0131 - val_loss: 0.0011 - val_mae: 0.0141\n",
            "Epoch 40/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0018 - mae: 0.0124 - val_loss: 0.0011 - val_mae: 0.0137\n",
            "Epoch 41/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0121 - val_loss: 0.0010 - val_mae: 0.0128\n",
            "Epoch 42/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0113 - val_loss: 9.3909e-04 - val_mae: 0.0123\n",
            "Epoch 43/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0111 - val_loss: 0.0010 - val_mae: 0.0129\n",
            "Epoch 44/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0109 - val_loss: 9.1586e-04 - val_mae: 0.0122\n",
            "Epoch 45/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0104 - val_loss: 8.4478e-04 - val_mae: 0.0111\n",
            "Epoch 46/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0100 - val_loss: 7.8169e-04 - val_mae: 0.0106\n",
            "Epoch 47/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0017 - mae: 0.0094 - val_loss: 7.9565e-04 - val_mae: 0.0102\n",
            "Epoch 48/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0096 - val_loss: 7.6209e-04 - val_mae: 0.0096\n",
            "Epoch 49/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0091 - val_loss: 6.7621e-04 - val_mae: 0.0094\n",
            "Epoch 50/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0086 - val_loss: 6.4942e-04 - val_mae: 0.0094\n",
            "Epoch 51/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0085 - val_loss: 6.1513e-04 - val_mae: 0.0091\n",
            "Epoch 52/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0087 - val_loss: 6.1730e-04 - val_mae: 0.0093\n",
            "Epoch 53/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0079 - val_loss: 6.1326e-04 - val_mae: 0.0093\n",
            "Epoch 54/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0080 - val_loss: 5.2366e-04 - val_mae: 0.0080\n",
            "Epoch 55/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0075 - val_loss: 5.0915e-04 - val_mae: 0.0079\n",
            "Epoch 56/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0073 - val_loss: 4.9380e-04 - val_mae: 0.0079\n",
            "Epoch 57/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0077 - val_loss: 4.9160e-04 - val_mae: 0.0079\n",
            "Epoch 58/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0070 - val_loss: 4.6605e-04 - val_mae: 0.0076\n",
            "Epoch 59/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0068 - val_loss: 4.9588e-04 - val_mae: 0.0080\n",
            "Epoch 60/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0071 - val_loss: 4.6075e-04 - val_mae: 0.0077\n",
            "Epoch 61/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0016 - mae: 0.0064 - val_loss: 3.9235e-04 - val_mae: 0.0067\n",
            "Epoch 62/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0062 - val_loss: 4.8815e-04 - val_mae: 0.0077\n",
            "Epoch 63/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0065 - val_loss: 3.6281e-04 - val_mae: 0.0064\n",
            "Epoch 64/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0061 - val_loss: 4.6247e-04 - val_mae: 0.0074\n",
            "Epoch 65/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0059 - val_loss: 3.3935e-04 - val_mae: 0.0062\n",
            "Epoch 66/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0058 - val_loss: 3.6221e-04 - val_mae: 0.0064\n",
            "Epoch 67/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0059 - val_loss: 3.1546e-04 - val_mae: 0.0058\n",
            "Epoch 68/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0057 - val_loss: 3.3992e-04 - val_mae: 0.0062\n",
            "Epoch 69/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0056 - val_loss: 3.2690e-04 - val_mae: 0.0060\n",
            "Epoch 70/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0054 - val_loss: 3.0438e-04 - val_mae: 0.0059\n",
            "Epoch 71/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0051 - val_loss: 3.2415e-04 - val_mae: 0.0060\n",
            "Epoch 72/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0054 - val_loss: 3.2876e-04 - val_mae: 0.0061\n",
            "Epoch 73/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0052 - val_loss: 5.5186e-04 - val_mae: 0.0073\n",
            "Epoch 74/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0055 - val_loss: 2.5800e-04 - val_mae: 0.0053\n",
            "Epoch 75/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0049 - val_loss: 3.5466e-04 - val_mae: 0.0060\n",
            "Epoch 76/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0050 - val_loss: 2.7909e-04 - val_mae: 0.0054\n",
            "Epoch 77/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0049 - val_loss: 2.6380e-04 - val_mae: 0.0052\n",
            "Epoch 78/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 3.0722e-04 - val_mae: 0.0055\n",
            "Epoch 79/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0049 - val_loss: 3.4828e-04 - val_mae: 0.0058\n",
            "Epoch 80/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0052 - val_loss: 2.1282e-04 - val_mae: 0.0046\n",
            "Epoch 81/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0048 - val_loss: 2.4794e-04 - val_mae: 0.0050\n",
            "Epoch 82/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0047 - val_loss: 2.4348e-04 - val_mae: 0.0048\n",
            "Epoch 83/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0047 - val_loss: 2.0906e-04 - val_mae: 0.0045\n",
            "Epoch 84/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0043 - val_loss: 2.5487e-04 - val_mae: 0.0049\n",
            "Epoch 85/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0047 - val_loss: 1.9739e-04 - val_mae: 0.0044\n",
            "Epoch 86/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0045 - val_loss: 2.1037e-04 - val_mae: 0.0045\n",
            "Epoch 87/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0044 - val_loss: 1.6588e-04 - val_mae: 0.0039\n",
            "Epoch 88/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0041 - val_loss: 1.7938e-04 - val_mae: 0.0042\n",
            "Epoch 89/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 1.7013e-04 - val_mae: 0.0041\n",
            "Epoch 90/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 1.8116e-04 - val_mae: 0.0043\n",
            "Epoch 91/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0044 - val_loss: 1.9384e-04 - val_mae: 0.0042\n",
            "Epoch 92/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0043 - val_loss: 1.7741e-04 - val_mae: 0.0041\n",
            "Epoch 93/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0041 - val_loss: 1.5975e-04 - val_mae: 0.0039\n",
            "Epoch 94/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0040 - val_loss: 1.7644e-04 - val_mae: 0.0040\n",
            "Epoch 95/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0040 - val_loss: 2.9580e-04 - val_mae: 0.0049\n",
            "Epoch 96/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0044 - val_loss: 4.5261e-04 - val_mae: 0.0059\n",
            "Epoch 97/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0048 - val_loss: 1.9639e-04 - val_mae: 0.0042\n",
            "Epoch 98/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0040 - val_loss: 1.4853e-04 - val_mae: 0.0037\n",
            "Epoch 99/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0040 - val_loss: 1.9940e-04 - val_mae: 0.0041\n",
            "Epoch 100/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0038 - val_loss: 1.7391e-04 - val_mae: 0.0038\n",
            "Epoch 101/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0040 - val_loss: 1.4866e-04 - val_mae: 0.0036\n",
            "Epoch 102/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0040 - val_loss: 1.3113e-04 - val_mae: 0.0035\n",
            "Epoch 103/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0040 - val_loss: 1.2560e-04 - val_mae: 0.0033\n",
            "Epoch 104/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0038 - val_loss: 1.7912e-04 - val_mae: 0.0038\n",
            "Epoch 105/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0038 - val_loss: 2.0246e-04 - val_mae: 0.0040\n",
            "Epoch 106/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0039 - val_loss: 1.7973e-04 - val_mae: 0.0038\n",
            "Epoch 107/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0038 - val_loss: 2.1674e-04 - val_mae: 0.0040\n",
            "Epoch 108/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0041 - val_loss: 2.1595e-04 - val_mae: 0.0040\n",
            "Epoch 109/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0036 - val_loss: 1.0153e-04 - val_mae: 0.0029\n",
            "Epoch 110/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0036 - val_loss: 1.6023e-04 - val_mae: 0.0034\n",
            "Epoch 111/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0034 - val_loss: 1.2740e-04 - val_mae: 0.0031\n",
            "Epoch 112/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0033 - val_loss: 1.1154e-04 - val_mae: 0.0029\n",
            "Epoch 113/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0035 - val_loss: 1.3874e-04 - val_mae: 0.0032\n",
            "Epoch 114/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.0015 - mae: 0.0033 - val_loss: 8.7710e-05 - val_mae: 0.0027\n",
            "Epoch 115/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0032 - val_loss: 1.3873e-04 - val_mae: 0.0031\n",
            "Epoch 116/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0034 - val_loss: 1.8577e-04 - val_mae: 0.0035\n",
            "Epoch 117/400\n",
            "105/105 [==============================] - 0s 4ms/step - loss: 0.0013 - mae: 0.0036 - val_loss: 1.6555e-04 - val_mae: 0.0033\n",
            "Epoch 118/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0035 - val_loss: 1.1455e-04 - val_mae: 0.0029\n",
            "Epoch 119/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0031 - val_loss: 1.0295e-04 - val_mae: 0.0027\n",
            "Epoch 120/400\n",
            "105/105 [==============================] - 1s 5ms/step - loss: 0.0015 - mae: 0.0030 - val_loss: 8.7180e-05 - val_mae: 0.0025\n",
            "Epoch 121/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0031 - val_loss: 1.5457e-04 - val_mae: 0.0031\n",
            "Epoch 122/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0031 - val_loss: 1.8639e-04 - val_mae: 0.0033\n",
            "Epoch 123/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0035 - val_loss: 1.6745e-04 - val_mae: 0.0032\n",
            "Epoch 124/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0032 - val_loss: 1.0412e-04 - val_mae: 0.0027\n",
            "Epoch 125/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0030 - val_loss: 1.2023e-04 - val_mae: 0.0028\n",
            "Epoch 126/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0032 - val_loss: 8.6318e-05 - val_mae: 0.0025\n",
            "Epoch 127/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0030 - val_loss: 7.7534e-05 - val_mae: 0.0023\n",
            "Epoch 128/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0030 - val_loss: 1.1644e-04 - val_mae: 0.0027\n",
            "Epoch 129/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0030 - val_loss: 1.1260e-04 - val_mae: 0.0026\n",
            "Epoch 130/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0029 - val_loss: 1.1802e-04 - val_mae: 0.0026\n",
            "Epoch 131/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0031 - val_loss: 9.9029e-05 - val_mae: 0.0024\n",
            "Epoch 132/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0029 - val_loss: 7.4725e-05 - val_mae: 0.0022\n",
            "Epoch 133/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0028 - val_loss: 9.0185e-05 - val_mae: 0.0023\n",
            "Epoch 134/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0028 - val_loss: 1.4600e-04 - val_mae: 0.0027\n",
            "Epoch 135/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0031 - val_loss: 1.2356e-04 - val_mae: 0.0026\n",
            "Epoch 136/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0029 - val_loss: 6.1559e-05 - val_mae: 0.0020\n",
            "Epoch 137/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0028 - val_loss: 7.6795e-05 - val_mae: 0.0022\n",
            "Epoch 138/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0029 - val_loss: 1.0208e-04 - val_mae: 0.0025\n",
            "Epoch 139/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0029 - val_loss: 8.1466e-05 - val_mae: 0.0022\n",
            "Epoch 140/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0029 - val_loss: 8.2991e-05 - val_mae: 0.0022\n",
            "Epoch 141/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0029 - val_loss: 9.2827e-05 - val_mae: 0.0023\n",
            "Epoch 142/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0028 - val_loss: 1.0828e-04 - val_mae: 0.0024\n",
            "Epoch 143/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0029 - val_loss: 8.0807e-05 - val_mae: 0.0021\n",
            "Epoch 144/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0028 - val_loss: 6.0074e-05 - val_mae: 0.0019\n",
            "Epoch 145/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0027 - val_loss: 7.4311e-05 - val_mae: 0.0021\n",
            "Epoch 146/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0027 - val_loss: 5.0629e-05 - val_mae: 0.0018\n",
            "Epoch 147/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0026 - val_loss: 4.5381e-05 - val_mae: 0.0017\n",
            "Epoch 148/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0026 - val_loss: 3.7098e-05 - val_mae: 0.0016\n",
            "Epoch 149/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0026 - val_loss: 5.8895e-05 - val_mae: 0.0018\n",
            "Epoch 150/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0027 - val_loss: 7.3114e-05 - val_mae: 0.0020\n",
            "Epoch 151/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0027 - val_loss: 7.1368e-05 - val_mae: 0.0020\n",
            "Epoch 152/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0027 - val_loss: 6.5553e-05 - val_mae: 0.0019\n",
            "Epoch 153/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0027 - val_loss: 7.1548e-05 - val_mae: 0.0020\n",
            "Epoch 154/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0027 - val_loss: 9.6193e-05 - val_mae: 0.0022\n",
            "Epoch 155/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0026 - val_loss: 7.5520e-05 - val_mae: 0.0020\n",
            "Epoch 156/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0028 - val_loss: 6.1007e-05 - val_mae: 0.0018\n",
            "Epoch 157/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0028 - val_loss: 1.1639e-04 - val_mae: 0.0023\n",
            "Epoch 158/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0026 - val_loss: 1.2925e-04 - val_mae: 0.0024\n",
            "Epoch 159/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0027 - val_loss: 1.5897e-04 - val_mae: 0.0025\n",
            "Epoch 160/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0027 - val_loss: 6.1203e-05 - val_mae: 0.0018\n",
            "Epoch 161/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0026 - val_loss: 1.2653e-04 - val_mae: 0.0023\n",
            "Epoch 162/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0028 - val_loss: 8.7726e-05 - val_mae: 0.0021\n",
            "Epoch 163/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0028 - val_loss: 5.9581e-05 - val_mae: 0.0018\n",
            "Epoch 164/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0027 - val_loss: 6.2173e-05 - val_mae: 0.0018\n",
            "Epoch 165/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 8.5959e-05 - val_mae: 0.0019\n",
            "Epoch 166/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 9.3122e-05 - val_mae: 0.0020\n",
            "Epoch 167/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0026 - val_loss: 9.4955e-05 - val_mae: 0.0021\n",
            "Epoch 168/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0027 - val_loss: 1.0957e-04 - val_mae: 0.0022\n",
            "Epoch 169/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0026 - val_loss: 7.5466e-05 - val_mae: 0.0019\n",
            "Epoch 170/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0026 - val_loss: 1.0676e-04 - val_mae: 0.0021\n",
            "Epoch 171/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0026 - val_loss: 7.6774e-05 - val_mae: 0.0019\n",
            "Epoch 172/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0026 - val_loss: 1.1956e-04 - val_mae: 0.0022\n",
            "Epoch 173/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0027 - val_loss: 1.3080e-04 - val_mae: 0.0022\n",
            "Epoch 174/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0024 - val_loss: 6.1014e-05 - val_mae: 0.0016\n",
            "Epoch 175/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0024 - val_loss: 8.1862e-05 - val_mae: 0.0018\n",
            "Epoch 176/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0024 - val_loss: 1.3600e-04 - val_mae: 0.0022\n",
            "Epoch 177/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 8.6093e-05 - val_mae: 0.0019\n",
            "Epoch 178/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 1.0458e-04 - val_mae: 0.0020\n",
            "Epoch 179/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 5.5786e-05 - val_mae: 0.0016\n",
            "Epoch 180/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 5.6324e-05 - val_mae: 0.0016\n",
            "Epoch 181/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 1.5768e-04 - val_mae: 0.0023\n",
            "Epoch 182/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0025 - val_loss: 1.3881e-04 - val_mae: 0.0022\n",
            "Epoch 183/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0024 - val_loss: 7.1235e-05 - val_mae: 0.0017\n",
            "Epoch 184/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0024 - val_loss: 4.0930e-05 - val_mae: 0.0014\n",
            "Epoch 185/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0024 - val_loss: 5.5831e-05 - val_mae: 0.0015\n",
            "Epoch 186/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 5.3758e-05 - val_mae: 0.0015\n",
            "Epoch 187/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0024 - val_loss: 6.6504e-05 - val_mae: 0.0016\n",
            "Epoch 188/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0024 - val_loss: 7.5207e-05 - val_mae: 0.0017\n",
            "Epoch 189/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 4.0110e-05 - val_mae: 0.0014\n",
            "Epoch 190/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0024 - val_loss: 5.0813e-05 - val_mae: 0.0015\n",
            "Epoch 191/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 3.5792e-05 - val_mae: 0.0014\n",
            "Epoch 192/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 8.2905e-05 - val_mae: 0.0018\n",
            "Epoch 193/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0025 - val_loss: 8.5751e-05 - val_mae: 0.0018\n",
            "Epoch 194/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0024 - val_loss: 1.1137e-04 - val_mae: 0.0020\n",
            "Epoch 195/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0026 - val_loss: 1.2632e-04 - val_mae: 0.0021\n",
            "Epoch 196/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0024 - val_loss: 5.5464e-05 - val_mae: 0.0015\n",
            "Epoch 197/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 9.1599e-05 - val_mae: 0.0018\n",
            "Epoch 198/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 6.6679e-05 - val_mae: 0.0016\n",
            "Epoch 199/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 8.9550e-05 - val_mae: 0.0017\n",
            "Epoch 200/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 4.1754e-05 - val_mae: 0.0013\n",
            "Epoch 201/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 1.0914e-04 - val_mae: 0.0018\n",
            "Epoch 202/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 5.1812e-05 - val_mae: 0.0014\n",
            "Epoch 203/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 4.6550e-05 - val_mae: 0.0014\n",
            "Epoch 204/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0023 - val_loss: 5.6587e-05 - val_mae: 0.0014\n",
            "Epoch 205/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 8.9103e-05 - val_mae: 0.0016\n",
            "Epoch 206/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0024 - val_loss: 9.6786e-05 - val_mae: 0.0017\n",
            "Epoch 207/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 5.9897e-05 - val_mae: 0.0014\n",
            "Epoch 208/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 6.7165e-05 - val_mae: 0.0015\n",
            "Epoch 209/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 7.0016e-05 - val_mae: 0.0015\n",
            "Epoch 210/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 6.9198e-05 - val_mae: 0.0015\n",
            "Epoch 211/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 7.8537e-05 - val_mae: 0.0016\n",
            "Epoch 212/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 6.1292e-05 - val_mae: 0.0014\n",
            "Epoch 213/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 7.4764e-05 - val_mae: 0.0016\n",
            "Epoch 214/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 4.2999e-05 - val_mae: 0.0013\n",
            "Epoch 215/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 4.5099e-05 - val_mae: 0.0013\n",
            "Epoch 216/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 6.3958e-05 - val_mae: 0.0015\n",
            "Epoch 217/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 7.7919e-05 - val_mae: 0.0015\n",
            "Epoch 218/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.1562e-04 - val_mae: 0.0017\n",
            "Epoch 219/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.1240e-04 - val_mae: 0.0017\n",
            "Epoch 220/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 9.4777e-05 - val_mae: 0.0016\n",
            "Epoch 221/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.0780e-04 - val_mae: 0.0017\n",
            "Epoch 222/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 6.5124e-05 - val_mae: 0.0014\n",
            "Epoch 223/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.0520e-04 - val_mae: 0.0017\n",
            "Epoch 224/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 8.8242e-05 - val_mae: 0.0016\n",
            "Epoch 225/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 8.4239e-05 - val_mae: 0.0016\n",
            "Epoch 226/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 6.0943e-05 - val_mae: 0.0014\n",
            "Epoch 227/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 6.1118e-05 - val_mae: 0.0014\n",
            "Epoch 228/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 5.8329e-05 - val_mae: 0.0014\n",
            "Epoch 229/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 6.7450e-05 - val_mae: 0.0014\n",
            "Epoch 230/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 5.6053e-05 - val_mae: 0.0013\n",
            "Epoch 231/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 7.7956e-05 - val_mae: 0.0015\n",
            "Epoch 232/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 7.9927e-05 - val_mae: 0.0015\n",
            "Epoch 233/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 6.5782e-05 - val_mae: 0.0014\n",
            "Epoch 234/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 8.0824e-05 - val_mae: 0.0015\n",
            "Epoch 235/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.2764e-04 - val_mae: 0.0018\n",
            "Epoch 236/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 7.8436e-05 - val_mae: 0.0015\n",
            "Epoch 237/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 9.0847e-05 - val_mae: 0.0016\n",
            "Epoch 238/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0023 - val_loss: 1.2683e-04 - val_mae: 0.0018\n",
            "Epoch 239/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.4397e-04 - val_mae: 0.0018\n",
            "Epoch 240/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0023 - val_loss: 2.1719e-04 - val_mae: 0.0021\n",
            "Epoch 241/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0022 - val_loss: 2.4477e-04 - val_mae: 0.0022\n",
            "Epoch 242/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0024 - val_loss: 4.2473e-04 - val_mae: 0.0029\n",
            "Epoch 243/400\n",
            "105/105 [==============================] - 1s 6ms/step - loss: 0.0011 - mae: 0.0024 - val_loss: 5.3000e-04 - val_mae: 0.0032\n",
            "Epoch 244/400\n",
            "105/105 [==============================] - 1s 6ms/step - loss: 0.0012 - mae: 0.0024 - val_loss: 3.8397e-04 - val_mae: 0.0028\n",
            "Epoch 245/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0023 - val_loss: 3.0410e-04 - val_mae: 0.0025\n",
            "Epoch 246/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0024 - val_loss: 2.6442e-04 - val_mae: 0.0024\n",
            "Epoch 247/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0024 - val_loss: 3.7098e-04 - val_mae: 0.0027\n",
            "Epoch 248/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0024 - val_loss: 2.0641e-04 - val_mae: 0.0021\n",
            "Epoch 249/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0023 - val_loss: 1.3556e-04 - val_mae: 0.0018\n",
            "Epoch 250/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0022 - val_loss: 2.0431e-04 - val_mae: 0.0021\n",
            "Epoch 251/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0023 - val_loss: 1.4132e-04 - val_mae: 0.0018\n",
            "Epoch 252/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.2830e-04 - val_mae: 0.0018\n",
            "Epoch 253/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 7.4515e-05 - val_mae: 0.0015\n",
            "Epoch 254/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 7.0364e-05 - val_mae: 0.0015\n",
            "Epoch 255/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0023 - val_loss: 7.2889e-05 - val_mae: 0.0015\n",
            "Epoch 256/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.1488e-04 - val_mae: 0.0017\n",
            "Epoch 257/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 7.3423e-05 - val_mae: 0.0015\n",
            "Epoch 258/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0022 - val_loss: 1.4683e-04 - val_mae: 0.0019\n",
            "Epoch 259/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0023 - val_loss: 2.4848e-04 - val_mae: 0.0022\n",
            "Epoch 260/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0022 - val_loss: 1.9880e-04 - val_mae: 0.0019\n",
            "Epoch 261/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0023 - val_loss: 2.9806e-04 - val_mae: 0.0023\n",
            "Epoch 262/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0023 - val_loss: 3.0352e-04 - val_mae: 0.0024\n",
            "Epoch 263/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0022 - val_loss: 9.4335e-05 - val_mae: 0.0015\n",
            "Epoch 264/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 1.7354e-04 - val_mae: 0.0019\n",
            "Epoch 265/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 1.6573e-04 - val_mae: 0.0018\n",
            "Epoch 266/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 2.2102e-04 - val_mae: 0.0020\n",
            "Epoch 267/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0022 - val_loss: 1.9167e-04 - val_mae: 0.0019\n",
            "Epoch 268/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0022 - val_loss: 2.5921e-04 - val_mae: 0.0022\n",
            "Epoch 269/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0021 - val_loss: 1.0083e-04 - val_mae: 0.0015\n",
            "Epoch 270/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 1.9086e-04 - val_mae: 0.0019\n",
            "Epoch 271/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 1.2007e-04 - val_mae: 0.0016\n",
            "Epoch 272/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 1.7779e-04 - val_mae: 0.0019\n",
            "Epoch 273/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0022 - val_loss: 1.6420e-04 - val_mae: 0.0018\n",
            "Epoch 274/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 1.7136e-04 - val_mae: 0.0019\n",
            "Epoch 275/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 1.6290e-04 - val_mae: 0.0018\n",
            "Epoch 276/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 1.3546e-04 - val_mae: 0.0017\n",
            "Epoch 277/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.0012 - mae: 0.0021 - val_loss: 1.1673e-04 - val_mae: 0.0016\n",
            "Epoch 278/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 1.4206e-04 - val_mae: 0.0016\n",
            "Epoch 279/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0020 - val_loss: 1.0831e-04 - val_mae: 0.0014\n",
            "Epoch 280/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 2.2045e-04 - val_mae: 0.0019\n",
            "Epoch 281/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0020 - val_loss: 2.4040e-04 - val_mae: 0.0020\n",
            "Epoch 282/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0021 - val_loss: 4.4892e-04 - val_mae: 0.0027\n",
            "Epoch 283/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0023 - val_loss: 3.3032e-04 - val_mae: 0.0023\n",
            "Epoch 284/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0022 - val_loss: 2.4390e-04 - val_mae: 0.0020\n",
            "Epoch 285/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 2.3163e-04 - val_mae: 0.0020\n",
            "Epoch 286/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0022 - val_loss: 1.5651e-04 - val_mae: 0.0017\n",
            "Epoch 287/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0021 - val_loss: 1.3748e-04 - val_mae: 0.0016\n",
            "Epoch 288/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 2.6930e-04 - val_mae: 0.0021\n",
            "Epoch 289/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 2.1560e-04 - val_mae: 0.0020\n",
            "Epoch 290/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0021 - val_loss: 2.2506e-04 - val_mae: 0.0019\n",
            "Epoch 291/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 3.6716e-04 - val_mae: 0.0024\n",
            "Epoch 292/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0022 - val_loss: 2.9825e-04 - val_mae: 0.0022\n",
            "Epoch 293/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 2.1749e-04 - val_mae: 0.0019\n",
            "Epoch 294/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 2.3484e-04 - val_mae: 0.0020\n",
            "Epoch 295/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0021 - val_loss: 2.7340e-04 - val_mae: 0.0021\n",
            "Epoch 296/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0021 - val_loss: 3.1558e-04 - val_mae: 0.0023\n",
            "Epoch 297/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0021 - val_loss: 3.6854e-04 - val_mae: 0.0024\n",
            "Epoch 298/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0020 - val_loss: 8.9087e-05 - val_mae: 0.0013\n",
            "Epoch 299/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 1.1493e-04 - val_mae: 0.0014\n",
            "Epoch 300/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0019 - val_loss: 8.6875e-05 - val_mae: 0.0012\n",
            "Epoch 301/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 1.8119e-04 - val_mae: 0.0017\n",
            "Epoch 302/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0020 - val_loss: 3.0463e-04 - val_mae: 0.0021\n",
            "Epoch 303/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0020 - val_loss: 2.9622e-04 - val_mae: 0.0021\n",
            "Epoch 304/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0021 - val_loss: 3.5115e-04 - val_mae: 0.0023\n",
            "Epoch 305/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0020 - val_loss: 2.6184e-04 - val_mae: 0.0020\n",
            "Epoch 306/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0021 - val_loss: 3.4433e-04 - val_mae: 0.0023\n",
            "Epoch 307/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0021 - val_loss: 1.8072e-04 - val_mae: 0.0017\n",
            "Epoch 308/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0020 - val_loss: 2.6271e-04 - val_mae: 0.0020\n",
            "Epoch 309/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0020 - val_loss: 2.3340e-04 - val_mae: 0.0018\n",
            "Epoch 310/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0020 - val_loss: 1.0209e-04 - val_mae: 0.0013\n",
            "Epoch 311/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 2.1002e-04 - val_mae: 0.0018\n",
            "Epoch 312/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0020 - val_loss: 2.3157e-04 - val_mae: 0.0019\n",
            "Epoch 313/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0020 - val_loss: 1.7775e-04 - val_mae: 0.0017\n",
            "Epoch 314/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 3.3528e-04 - val_mae: 0.0022\n",
            "Epoch 315/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0020 - val_loss: 5.3487e-04 - val_mae: 0.0027\n",
            "Epoch 316/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0020 - val_loss: 1.8638e-04 - val_mae: 0.0017\n",
            "Epoch 317/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 3.6163e-04 - val_mae: 0.0023\n",
            "Epoch 318/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0020 - val_loss: 4.6246e-04 - val_mae: 0.0025\n",
            "Epoch 319/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0020 - val_loss: 3.9173e-04 - val_mae: 0.0023\n",
            "Epoch 320/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0020 - val_loss: 4.7266e-04 - val_mae: 0.0026\n",
            "Epoch 321/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 9.3566e-04 - mae: 0.0020 - val_loss: 4.7076e-04 - val_mae: 0.0025\n",
            "Epoch 322/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0020 - val_loss: 8.7807e-05 - val_mae: 0.0012\n",
            "Epoch 323/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.2429e-04 - val_mae: 0.0014\n",
            "Epoch 324/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 1.4392e-04 - val_mae: 0.0015\n",
            "Epoch 325/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0019 - val_loss: 6.2267e-05 - val_mae: 0.0010\n",
            "Epoch 326/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 1.3164e-04 - val_mae: 0.0014\n",
            "Epoch 327/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0019 - val_loss: 7.7024e-05 - val_mae: 0.0011\n",
            "Epoch 328/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 1.1865e-04 - val_mae: 0.0013\n",
            "Epoch 329/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 1.9770e-04 - val_mae: 0.0017\n",
            "Epoch 330/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 1.6497e-04 - val_mae: 0.0016\n",
            "Epoch 331/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0020 - val_loss: 2.0265e-04 - val_mae: 0.0017\n",
            "Epoch 332/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 1.5510e-04 - val_mae: 0.0015\n",
            "Epoch 333/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 2.1948e-04 - val_mae: 0.0018\n",
            "Epoch 334/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0020 - val_loss: 2.2775e-04 - val_mae: 0.0018\n",
            "Epoch 335/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.9281e-04 - val_mae: 0.0016\n",
            "Epoch 336/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 3.2270e-04 - val_mae: 0.0021\n",
            "Epoch 337/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 6.8199e-04 - val_mae: 0.0029\n",
            "Epoch 338/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0020 - val_loss: 2.0415e-04 - val_mae: 0.0017\n",
            "Epoch 339/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0020 - val_loss: 3.0214e-04 - val_mae: 0.0020\n",
            "Epoch 340/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0020 - val_loss: 2.3807e-04 - val_mae: 0.0018\n",
            "Epoch 341/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 2.1327e-04 - val_mae: 0.0017\n",
            "Epoch 342/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 3.8593e-04 - val_mae: 0.0022\n",
            "Epoch 343/400\n",
            "105/105 [==============================] - 0s 3ms/step - loss: 0.0010 - mae: 0.0020 - val_loss: 4.2524e-04 - val_mae: 0.0024\n",
            "Epoch 344/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 1.0677e-04 - val_mae: 0.0013\n",
            "Epoch 345/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.9521e-04 - val_mae: 0.0016\n",
            "Epoch 346/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 1.9238e-04 - val_mae: 0.0016\n",
            "Epoch 347/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.8074e-04 - val_mae: 0.0016\n",
            "Epoch 348/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 9.9000e-05 - val_mae: 0.0012\n",
            "Epoch 349/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0018 - val_loss: 1.5149e-04 - val_mae: 0.0014\n",
            "Epoch 350/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 2.3825e-04 - val_mae: 0.0018\n",
            "Epoch 351/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0019 - val_loss: 8.6647e-05 - val_mae: 0.0011\n",
            "Epoch 352/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.0957e-04 - val_mae: 0.0012\n",
            "Epoch 353/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0018 - val_loss: 4.1168e-05 - val_mae: 8.5739e-04\n",
            "Epoch 354/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0019 - val_loss: 9.5106e-05 - val_mae: 0.0012\n",
            "Epoch 355/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.3799e-04 - val_mae: 0.0014\n",
            "Epoch 356/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0018 - val_loss: 8.1921e-05 - val_mae: 0.0011\n",
            "Epoch 357/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.3786e-04 - val_mae: 0.0014\n",
            "Epoch 358/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0019 - val_loss: 4.8128e-05 - val_mae: 9.1408e-04\n",
            "Epoch 359/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0015 - mae: 0.0018 - val_loss: 4.7962e-05 - val_mae: 9.2382e-04\n",
            "Epoch 360/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0018 - val_loss: 9.0786e-05 - val_mae: 0.0012\n",
            "Epoch 361/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.8426e-04 - val_mae: 0.0015\n",
            "Epoch 362/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 1.5900e-04 - val_mae: 0.0014\n",
            "Epoch 363/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0018 - val_loss: 1.7677e-04 - val_mae: 0.0015\n",
            "Epoch 364/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0018 - val_loss: 1.6081e-04 - val_mae: 0.0014\n",
            "Epoch 365/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0018 - val_loss: 2.0552e-04 - val_mae: 0.0016\n",
            "Epoch 366/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0018 - val_loss: 1.8194e-04 - val_mae: 0.0015\n",
            "Epoch 367/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0018 - val_loss: 3.8588e-04 - val_mae: 0.0021\n",
            "Epoch 368/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0019 - val_loss: 6.8756e-04 - val_mae: 0.0028\n",
            "Epoch 369/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 9.3427e-04 - mae: 0.0019 - val_loss: 8.4030e-04 - val_mae: 0.0032\n",
            "Epoch 370/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 9.9307e-04 - mae: 0.0019 - val_loss: 5.4136e-04 - val_mae: 0.0025\n",
            "Epoch 371/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 9.3100e-04 - mae: 0.0019 - val_loss: 7.6003e-04 - val_mae: 0.0030\n",
            "Epoch 372/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 3.8716e-04 - val_mae: 0.0021\n",
            "Epoch 373/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0019 - val_loss: 4.8846e-04 - val_mae: 0.0023\n",
            "Epoch 374/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 3.7493e-04 - val_mae: 0.0020\n",
            "Epoch 375/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 5.2442e-04 - val_mae: 0.0024\n",
            "Epoch 376/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 9.7178e-04 - mae: 0.0019 - val_loss: 4.7838e-04 - val_mae: 0.0023\n",
            "Epoch 377/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0019 - val_loss: 4.6696e-04 - val_mae: 0.0023\n",
            "Epoch 378/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0019 - val_loss: 4.4593e-04 - val_mae: 0.0023\n",
            "Epoch 379/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 9.6814e-04 - mae: 0.0019 - val_loss: 5.3684e-04 - val_mae: 0.0024\n",
            "Epoch 380/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 5.2962e-04 - val_mae: 0.0024\n",
            "Epoch 381/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0019 - val_loss: 3.8222e-04 - val_mae: 0.0020\n",
            "Epoch 382/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0018 - val_loss: 6.6322e-04 - val_mae: 0.0027\n",
            "Epoch 383/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0018 - val_loss: 2.3026e-04 - val_mae: 0.0016\n",
            "Epoch 384/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0018 - val_loss: 4.6739e-04 - val_mae: 0.0023\n",
            "Epoch 385/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0018 - val_loss: 6.4559e-04 - val_mae: 0.0027\n",
            "Epoch 386/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 8.4000e-04 - mae: 0.0018 - val_loss: 4.9035e-04 - val_mae: 0.0023\n",
            "Epoch 387/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0019 - val_loss: 3.2374e-04 - val_mae: 0.0019\n",
            "Epoch 388/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0018 - val_loss: 3.2475e-04 - val_mae: 0.0019\n",
            "Epoch 389/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0018 - val_loss: 4.5933e-04 - val_mae: 0.0022\n",
            "Epoch 390/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0019 - val_loss: 5.2545e-04 - val_mae: 0.0024\n",
            "Epoch 391/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0018 - val_loss: 1.8947e-04 - val_mae: 0.0015\n",
            "Epoch 392/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0018 - val_loss: 2.6961e-04 - val_mae: 0.0017\n",
            "Epoch 393/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0018 - val_loss: 3.7882e-04 - val_mae: 0.0020\n",
            "Epoch 394/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0018 - val_loss: 3.6403e-04 - val_mae: 0.0020\n",
            "Epoch 395/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0010 - mae: 0.0019 - val_loss: 4.0138e-04 - val_mae: 0.0021\n",
            "Epoch 396/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0018 - val_loss: 4.9567e-04 - val_mae: 0.0023\n",
            "Epoch 397/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0013 - mae: 0.0018 - val_loss: 3.0251e-04 - val_mae: 0.0018\n",
            "Epoch 398/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0014 - mae: 0.0018 - val_loss: 1.7619e-04 - val_mae: 0.0014\n",
            "Epoch 399/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0012 - mae: 0.0017 - val_loss: 3.5250e-04 - val_mae: 0.0020\n",
            "Epoch 400/400\n",
            "105/105 [==============================] - 0s 2ms/step - loss: 0.0011 - mae: 0.0018 - val_loss: 5.4919e-04 - val_mae: 0.0024\n"
          ]
        }
      ],
      "source": [
        "# build the model and train it\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu')) # relu is used for performance\n",
        "model.add(tf.keras.layers.Dense(6, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(6, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')) # softmax is used, because we only expect one class to occur per input\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit(inputs_train, outputs_train, epochs=400, batch_size=4, validation_data=(inputs_validate, outputs_validate))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMjtfa42ahM"
      },
      "source": [
        "### Run with Test Data\n",
        "Put our test data into the model and plot the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "V3Y0CCWJz2EK",
        "outputId": "e09dee93-92d3-4af8-fdb3-2cf21c5f62b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5/5 [==============================] - 0s 2ms/step\n",
            "predictions =\n",
            " [[1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.287 0.713 0.   ]\n",
            " [0.999 0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.001 0.    0.999]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.002 0.    0.998]\n",
            " [1.    0.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.002 0.    0.998]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.003 0.    0.997]\n",
            " [0.002 0.    0.998]\n",
            " [0.995 0.002 0.003]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.003 0.    0.997]\n",
            " [0.003 0.997 0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.998 0.001 0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.001 0.    0.999]\n",
            " [1.    0.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.002 0.    0.998]\n",
            " [0.001 0.    0.999]\n",
            " [0.002 0.    0.998]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [0.    1.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.001 0.    0.999]\n",
            " [1.    0.    0.   ]\n",
            " [0.003 0.997 0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [1.    0.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.    1.    0.   ]\n",
            " [0.001 0.    0.999]]\n",
            "actual =\n",
            " [[1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x72 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(inputs_test)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
        "print(\"actual =\\n\", outputs_test)\n",
        "\n",
        "# Plot the predictions along with to the test data\n",
        "plt.clf()\n",
        "plt.title('Training data predicted vs actual values')\n",
        "plt.plot(inputs_test, outputs_test, 'b.', label='Actual')\n",
        "plt.plot(inputs_test, predictions, 'r.', label='Predicted')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DO6xxXVCym"
      },
      "source": [
        "# Convert the Trained Model to Tensor Flow Lite\n",
        "\n",
        "The next cell converts the model to TFlite format. The size in bytes of the model is also printed out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xn1-Rn9Cp_8",
        "outputId": "ded7fa14-99ae-487d-d98c-b7cc829d3b2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is 2464 bytes\n"
          ]
        }
      ],
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
        "  \n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)\n",
        "  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykccQn7SXrUX"
      },
      "source": [
        "## Encode the Model in an Arduino Header File \n",
        "\n",
        "The next cell creates a constant byte array that contains the TFlite model. Import it as a tab with the sketch below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9J33uwpNtAku",
        "outputId": "5dfff3bb-b891-4acf-a4fe-312c77dbf25c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Header file, model.h, is 15,230 bytes.\n",
            "\n",
            "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
          ]
        }
      ],
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat gesture_model.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSkHZaLzMId"
      },
      "source": [
        "# Realtime Classification of Sensor Data on Arduino\n",
        "\n",
        "Now it's time to switch back to the tutorial instructions and run our new model on the [Arduino Nano 33 BLE Sense](https://www.arduino.cc/en/Guide/NANO33BLE)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
