{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FruitToEmoji-GIT.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f92-4Hjy7kA8"
      },
      "source": [
        "<a href=\"https://www.arduino.cc/\"><img src=\"https://raw.githubusercontent.com/sandeepmistry/aimldevfest-workshop-2019/master/images/Arduino_logo_R_highquality.png\" width=200/></a>\n",
        "# Tiny ML on Arduino\n",
        "## Classify objects by color tutorial\n",
        "\n",
        "\n",
        "https://github.com/arduino/ArduinoTensorFlowLiteTutorials/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvDA8AK7QOq-"
      },
      "source": [
        "## Setup Python Environment\n",
        "\n",
        "The next cell sets up the dependencies in required for the notebook, run it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2gs-PL4xDkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4276fbc-24aa-4642-c52d-d9db04bad764"
      },
      "source": [
        "# Setup environment\n",
        "!apt-get -qq install xxd\n",
        "!pip install pandas numpy matplotlib\n",
        "%tensorflow_version 2.x\n",
        "!pip install tensorflow"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.0)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.25.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.1)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lwkeshJk7dg"
      },
      "source": [
        "# Upload Data\n",
        "\n",
        "1. Open the panel on the left side of Colab by clicking on the __>__\n",
        "1. Select the Files tab\n",
        "1. Drag `csv` files from your computer to the tab to upload them into colab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSxUeYPNQbOg"
      },
      "source": [
        "# Train Neural Network\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gxk414PU3oy3"
      },
      "source": [
        "## Parse and prepare the data\n",
        "\n",
        "The next cell parses the csv files and transforms them to a format that will be used to train the full connected neural network.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGChd1FAk5_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "outputId": "bd6d3a74-e20f-4e59-fffa-41781b11de4b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import fileinput\n",
        "\n",
        "print(f\"TensorFlow version = {tf.__version__}\\n\")\n",
        "\n",
        "# Set a fixed random seed value, for reproducibility, this will allow us to get\n",
        "# the same random numbers each time the notebook is run\n",
        "SEED = 1337\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "\n",
        "CLASSES = [];\n",
        "\n",
        "for file in os.listdir(\"/content/\"):\n",
        "    if file.endswith(\".csv\"):\n",
        "        CLASSES.append(os.path.splitext(file)[0])\n",
        "\n",
        "CLASSES.sort()\n",
        "\n",
        "SAMPLES_WINDOW_LEN = 1\n",
        "NUM_CLASSES = len(CLASSES)\n",
        "\n",
        "# create a one-hot encoded matrix that is used in the output\n",
        "ONE_HOT_ENCODED_CLASSES = np.eye(NUM_CLASSES)\n",
        "\n",
        "inputs = []\n",
        "outputs = []\n",
        "\n",
        "# read each csv file and push an input and output\n",
        "for class_index in range(NUM_CLASSES):\n",
        "  objectClass = CLASSES[class_index]\n",
        "  df = pd.read_csv(\"/content/\" + objectClass + \".csv\")\n",
        "  columns = list(df)\n",
        "  # get rid of pesky empty value lines of csv which cause NaN inputs to TensorFlow\n",
        "  df = df.dropna()\n",
        "  df = df.reset_index(drop=True)\n",
        "\n",
        "  # calculate the number of objectClass recordings in the file\n",
        "  num_recordings = int(df.shape[0] / SAMPLES_WINDOW_LEN)\n",
        "  print(f\"\\u001b[32;4m{objectClass}\\u001b[0m class will be output \\u001b[32m{class_index}\\u001b[0m of the classifier\")\n",
        "  print(f\"{num_recordings} samples captured for training with inputs {list(df)} \\n\")\n",
        "\n",
        "  # graphing\n",
        "  plt.rcParams[\"figure.figsize\"] = (10,1)\n",
        "  pixels = np.array([df['Red'],df['Green'],df['Blue']],float)\n",
        "  pixels = np.transpose(pixels)\n",
        "  for i in range(num_recordings):\n",
        "    plt.axvline(x=i, linewidth=8, color=tuple(pixels[i]/np.max(pixels[i], axis=0)))\n",
        "  plt.show()\n",
        "\n",
        "  #tensors\n",
        "  output = ONE_HOT_ENCODED_CLASSES[class_index]\n",
        "  for i in range(num_recordings):\n",
        "    tensor = []\n",
        "    row = []\n",
        "    for c in columns:\n",
        "      row.append(df[c][i])\n",
        "    tensor += row\n",
        "    inputs.append(tensor)\n",
        "    outputs.append(output)\n",
        "\n",
        "# convert the list to numpy array\n",
        "inputs = np.array(inputs)\n",
        "outputs = np.array(outputs)\n",
        "\n",
        "print(\"Data set parsing and preparation complete.\")\n",
        "\n",
        "# Randomize the order of the inputs, so they can be evenly distributed for training, testing, and validation\n",
        "# https://stackoverflow.com/a/37710486/2020087\n",
        "num_inputs = len(inputs)\n",
        "randomize = np.arange(num_inputs)\n",
        "np.random.shuffle(randomize)\n",
        "\n",
        "# Swap the consecutive indexes (0, 1, 2, etc) with the randomized indexes\n",
        "inputs = inputs[randomize]\n",
        "outputs = outputs[randomize]\n",
        "\n",
        "# Split the recordings (group of samples) into three sets: training, testing and validation\n",
        "TRAIN_SPLIT = int(0.6 * num_inputs)\n",
        "TEST_SPLIT = int(0.2 * num_inputs + TRAIN_SPLIT)\n",
        "\n",
        "inputs_train, inputs_test, inputs_validate = np.split(inputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "outputs_train, outputs_test, outputs_validate = np.split(outputs, [TRAIN_SPLIT, TEST_SPLIT])\n",
        "\n",
        "print(\"Data set randomization and splitting complete.\")\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version = 2.15.0\n",
            "\n",
            "\u001b[32;4mapple\u001b[0m class will be output \u001b[32m0\u001b[0m of the classifier\n",
            "217 samples captured for training with inputs ['Red', 'Green', 'Blue'] \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;4mbanana\u001b[0m class will be output \u001b[32m1\u001b[0m of the classifier\n",
            "217 samples captured for training with inputs ['Red', 'Green', 'Blue'] \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[32;4morange\u001b[0m class will be output \u001b[32m2\u001b[0m of the classifier\n",
            "123 samples captured for training with inputs ['Red', 'Green', 'Blue'] \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data set parsing and preparation complete.\n",
            "Data set randomization and splitting complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8qlSAX1b6Yv"
      },
      "source": [
        "## Build & Train the Model\n",
        "\n",
        "Build and train a [TensorFlow](https://www.tensorflow.org) model using the high-level [Keras](https://www.tensorflow.org/guide/keras) API."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kGNFa-lX24Qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d75b7bd-669e-4e1d-f065-ba1656bccb2e"
      },
      "source": [
        "# build the model and train it\n",
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(8, activation='relu')) # relu is used for performance\n",
        "model.add(tf.keras.layers.Dense(5, activation='relu'))\n",
        "model.add(tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')) # softmax is used, because we only expect one class to occur per input\n",
        "model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])\n",
        "history = model.fit(inputs_train, outputs_train, epochs=400, batch_size=4, validation_data=(inputs_validate, outputs_validate))\n",
        "\n"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/400\n",
            "84/84 [==============================] - 1s 4ms/step - loss: 0.2169 - mae: 0.4360 - val_loss: 0.2197 - val_mae: 0.4382\n",
            "Epoch 2/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.2143 - mae: 0.4321 - val_loss: 0.2182 - val_mae: 0.4361\n",
            "Epoch 3/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.2127 - mae: 0.4298 - val_loss: 0.2165 - val_mae: 0.4337\n",
            "Epoch 4/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.2106 - mae: 0.4262 - val_loss: 0.2136 - val_mae: 0.4312\n",
            "Epoch 5/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.2082 - mae: 0.4251 - val_loss: 0.2102 - val_mae: 0.4274\n",
            "Epoch 6/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.2051 - mae: 0.4211 - val_loss: 0.2057 - val_mae: 0.4232\n",
            "Epoch 7/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.2014 - mae: 0.4186 - val_loss: 0.2006 - val_mae: 0.4169\n",
            "Epoch 8/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1963 - mae: 0.4119 - val_loss: 0.1938 - val_mae: 0.4093\n",
            "Epoch 9/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1905 - mae: 0.4044 - val_loss: 0.1874 - val_mae: 0.4007\n",
            "Epoch 10/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1854 - mae: 0.3966 - val_loss: 0.1817 - val_mae: 0.3921\n",
            "Epoch 11/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1803 - mae: 0.3890 - val_loss: 0.1763 - val_mae: 0.3835\n",
            "Epoch 12/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1751 - mae: 0.3807 - val_loss: 0.1687 - val_mae: 0.3740\n",
            "Epoch 13/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1701 - mae: 0.3731 - val_loss: 0.1623 - val_mae: 0.3643\n",
            "Epoch 14/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1649 - mae: 0.3645 - val_loss: 0.1557 - val_mae: 0.3539\n",
            "Epoch 15/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1599 - mae: 0.3558 - val_loss: 0.1498 - val_mae: 0.3433\n",
            "Epoch 16/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1556 - mae: 0.3472 - val_loss: 0.1445 - val_mae: 0.3330\n",
            "Epoch 17/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1519 - mae: 0.3395 - val_loss: 0.1397 - val_mae: 0.3234\n",
            "Epoch 18/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1486 - mae: 0.3320 - val_loss: 0.1361 - val_mae: 0.3151\n",
            "Epoch 19/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1458 - mae: 0.3246 - val_loss: 0.1325 - val_mae: 0.3064\n",
            "Epoch 20/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1439 - mae: 0.3189 - val_loss: 0.1309 - val_mae: 0.3008\n",
            "Epoch 21/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1425 - mae: 0.3141 - val_loss: 0.1289 - val_mae: 0.2950\n",
            "Epoch 22/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1413 - mae: 0.3097 - val_loss: 0.1273 - val_mae: 0.2898\n",
            "Epoch 23/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1404 - mae: 0.3058 - val_loss: 0.1264 - val_mae: 0.2859\n",
            "Epoch 24/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1397 - mae: 0.3028 - val_loss: 0.1254 - val_mae: 0.2824\n",
            "Epoch 25/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1391 - mae: 0.3001 - val_loss: 0.1251 - val_mae: 0.2805\n",
            "Epoch 26/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1385 - mae: 0.2975 - val_loss: 0.1246 - val_mae: 0.2779\n",
            "Epoch 27/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1380 - mae: 0.2950 - val_loss: 0.1238 - val_mae: 0.2751\n",
            "Epoch 28/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1378 - mae: 0.2931 - val_loss: 0.1234 - val_mae: 0.2732\n",
            "Epoch 29/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1374 - mae: 0.2920 - val_loss: 0.1230 - val_mae: 0.2704\n",
            "Epoch 30/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1370 - mae: 0.2899 - val_loss: 0.1239 - val_mae: 0.2727\n",
            "Epoch 31/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1372 - mae: 0.2892 - val_loss: 0.1232 - val_mae: 0.2701\n",
            "Epoch 32/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1368 - mae: 0.2883 - val_loss: 0.1218 - val_mae: 0.2650\n",
            "Epoch 33/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1366 - mae: 0.2867 - val_loss: 0.1218 - val_mae: 0.2648\n",
            "Epoch 34/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1359 - mae: 0.2850 - val_loss: 0.1216 - val_mae: 0.2622\n",
            "Epoch 35/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1364 - mae: 0.2842 - val_loss: 0.1219 - val_mae: 0.2652\n",
            "Epoch 36/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1362 - mae: 0.2846 - val_loss: 0.1213 - val_mae: 0.2621\n",
            "Epoch 37/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1359 - mae: 0.2833 - val_loss: 0.1211 - val_mae: 0.2611\n",
            "Epoch 38/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1360 - mae: 0.2830 - val_loss: 0.1209 - val_mae: 0.2598\n",
            "Epoch 39/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1360 - mae: 0.2822 - val_loss: 0.1211 - val_mae: 0.2608\n",
            "Epoch 40/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1357 - mae: 0.2823 - val_loss: 0.1208 - val_mae: 0.2582\n",
            "Epoch 41/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1357 - mae: 0.2809 - val_loss: 0.1208 - val_mae: 0.2586\n",
            "Epoch 42/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1357 - mae: 0.2804 - val_loss: 0.1210 - val_mae: 0.2596\n",
            "Epoch 43/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1356 - mae: 0.2803 - val_loss: 0.1206 - val_mae: 0.2576\n",
            "Epoch 44/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1356 - mae: 0.2800 - val_loss: 0.1206 - val_mae: 0.2574\n",
            "Epoch 45/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1354 - mae: 0.2796 - val_loss: 0.1204 - val_mae: 0.2558\n",
            "Epoch 46/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1353 - mae: 0.2787 - val_loss: 0.1206 - val_mae: 0.2560\n",
            "Epoch 47/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1353 - mae: 0.2780 - val_loss: 0.1209 - val_mae: 0.2571\n",
            "Epoch 48/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1352 - mae: 0.2784 - val_loss: 0.1202 - val_mae: 0.2542\n",
            "Epoch 49/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1353 - mae: 0.2778 - val_loss: 0.1204 - val_mae: 0.2550\n",
            "Epoch 50/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1351 - mae: 0.2772 - val_loss: 0.1202 - val_mae: 0.2543\n",
            "Epoch 51/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1351 - mae: 0.2770 - val_loss: 0.1201 - val_mae: 0.2538\n",
            "Epoch 52/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1349 - mae: 0.2774 - val_loss: 0.1200 - val_mae: 0.2525\n",
            "Epoch 53/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1350 - mae: 0.2762 - val_loss: 0.1200 - val_mae: 0.2526\n",
            "Epoch 54/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1348 - mae: 0.2761 - val_loss: 0.1199 - val_mae: 0.2524\n",
            "Epoch 55/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1348 - mae: 0.2758 - val_loss: 0.1199 - val_mae: 0.2521\n",
            "Epoch 56/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1349 - mae: 0.2758 - val_loss: 0.1202 - val_mae: 0.2533\n",
            "Epoch 57/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1345 - mae: 0.2753 - val_loss: 0.1209 - val_mae: 0.2558\n",
            "Epoch 58/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1347 - mae: 0.2760 - val_loss: 0.1203 - val_mae: 0.2525\n",
            "Epoch 59/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1347 - mae: 0.2742 - val_loss: 0.1202 - val_mae: 0.2526\n",
            "Epoch 60/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1346 - mae: 0.2754 - val_loss: 0.1202 - val_mae: 0.2519\n",
            "Epoch 61/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1344 - mae: 0.2740 - val_loss: 0.1209 - val_mae: 0.2539\n",
            "Epoch 62/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1342 - mae: 0.2750 - val_loss: 0.1205 - val_mae: 0.2513\n",
            "Epoch 63/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1343 - mae: 0.2739 - val_loss: 0.1209 - val_mae: 0.2539\n",
            "Epoch 64/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1343 - mae: 0.2736 - val_loss: 0.1209 - val_mae: 0.2533\n",
            "Epoch 65/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1343 - mae: 0.2740 - val_loss: 0.1207 - val_mae: 0.2529\n",
            "Epoch 66/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1342 - mae: 0.2733 - val_loss: 0.1206 - val_mae: 0.2520\n",
            "Epoch 67/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1340 - mae: 0.2734 - val_loss: 0.1203 - val_mae: 0.2498\n",
            "Epoch 68/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1343 - mae: 0.2723 - val_loss: 0.1203 - val_mae: 0.2500\n",
            "Epoch 69/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1343 - mae: 0.2725 - val_loss: 0.1202 - val_mae: 0.2501\n",
            "Epoch 70/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1342 - mae: 0.2722 - val_loss: 0.1203 - val_mae: 0.2511\n",
            "Epoch 71/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1339 - mae: 0.2730 - val_loss: 0.1201 - val_mae: 0.2495\n",
            "Epoch 72/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1340 - mae: 0.2721 - val_loss: 0.1206 - val_mae: 0.2511\n",
            "Epoch 73/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1339 - mae: 0.2714 - val_loss: 0.1208 - val_mae: 0.2525\n",
            "Epoch 74/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1340 - mae: 0.2729 - val_loss: 0.1203 - val_mae: 0.2495\n",
            "Epoch 75/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1338 - mae: 0.2709 - val_loss: 0.1208 - val_mae: 0.2519\n",
            "Epoch 76/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1339 - mae: 0.2725 - val_loss: 0.1206 - val_mae: 0.2506\n",
            "Epoch 77/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1340 - mae: 0.2721 - val_loss: 0.1203 - val_mae: 0.2485\n",
            "Epoch 78/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1339 - mae: 0.2710 - val_loss: 0.1202 - val_mae: 0.2491\n",
            "Epoch 79/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1340 - mae: 0.2714 - val_loss: 0.1202 - val_mae: 0.2492\n",
            "Epoch 80/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1340 - mae: 0.2712 - val_loss: 0.1202 - val_mae: 0.2492\n",
            "Epoch 81/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1340 - mae: 0.2713 - val_loss: 0.1204 - val_mae: 0.2507\n",
            "Epoch 82/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1339 - mae: 0.2717 - val_loss: 0.1203 - val_mae: 0.2500\n",
            "Epoch 83/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1339 - mae: 0.2718 - val_loss: 0.1203 - val_mae: 0.2497\n",
            "Epoch 84/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2717 - val_loss: 0.1202 - val_mae: 0.2488\n",
            "Epoch 85/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1337 - mae: 0.2708 - val_loss: 0.1200 - val_mae: 0.2471\n",
            "Epoch 86/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1340 - mae: 0.2707 - val_loss: 0.1201 - val_mae: 0.2477\n",
            "Epoch 87/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1338 - mae: 0.2707 - val_loss: 0.1199 - val_mae: 0.2470\n",
            "Epoch 88/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1339 - mae: 0.2695 - val_loss: 0.1203 - val_mae: 0.2478\n",
            "Epoch 89/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1339 - mae: 0.2700 - val_loss: 0.1206 - val_mae: 0.2489\n",
            "Epoch 90/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1339 - mae: 0.2703 - val_loss: 0.1203 - val_mae: 0.2492\n",
            "Epoch 91/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1339 - mae: 0.2711 - val_loss: 0.1202 - val_mae: 0.2482\n",
            "Epoch 92/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2700 - val_loss: 0.1199 - val_mae: 0.2468\n",
            "Epoch 93/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1340 - mae: 0.2708 - val_loss: 0.1200 - val_mae: 0.2472\n",
            "Epoch 94/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1338 - mae: 0.2698 - val_loss: 0.1201 - val_mae: 0.2485\n",
            "Epoch 95/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1338 - mae: 0.2700 - val_loss: 0.1201 - val_mae: 0.2475\n",
            "Epoch 96/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1339 - mae: 0.2704 - val_loss: 0.1202 - val_mae: 0.2479\n",
            "Epoch 97/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1338 - mae: 0.2699 - val_loss: 0.1203 - val_mae: 0.2481\n",
            "Epoch 98/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1338 - mae: 0.2699 - val_loss: 0.1203 - val_mae: 0.2483\n",
            "Epoch 99/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1338 - mae: 0.2701 - val_loss: 0.1201 - val_mae: 0.2468\n",
            "Epoch 100/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1338 - mae: 0.2698 - val_loss: 0.1201 - val_mae: 0.2466\n",
            "Epoch 101/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1338 - mae: 0.2694 - val_loss: 0.1203 - val_mae: 0.2482\n",
            "Epoch 102/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1338 - mae: 0.2700 - val_loss: 0.1203 - val_mae: 0.2481\n",
            "Epoch 103/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1339 - mae: 0.2697 - val_loss: 0.1202 - val_mae: 0.2472\n",
            "Epoch 104/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1338 - mae: 0.2691 - val_loss: 0.1203 - val_mae: 0.2481\n",
            "Epoch 105/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2692 - val_loss: 0.1201 - val_mae: 0.2476\n",
            "Epoch 106/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2695 - val_loss: 0.1208 - val_mae: 0.2499\n",
            "Epoch 107/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1338 - mae: 0.2703 - val_loss: 0.1202 - val_mae: 0.2468\n",
            "Epoch 108/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1338 - mae: 0.2688 - val_loss: 0.1202 - val_mae: 0.2475\n",
            "Epoch 109/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2700 - val_loss: 0.1202 - val_mae: 0.2473\n",
            "Epoch 110/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1338 - mae: 0.2695 - val_loss: 0.1201 - val_mae: 0.2468\n",
            "Epoch 111/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2694 - val_loss: 0.1201 - val_mae: 0.2464\n",
            "Epoch 112/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2692 - val_loss: 0.1203 - val_mae: 0.2478\n",
            "Epoch 113/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2695 - val_loss: 0.1201 - val_mae: 0.2466\n",
            "Epoch 114/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1336 - mae: 0.2691 - val_loss: 0.1200 - val_mae: 0.2458\n",
            "Epoch 115/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1338 - mae: 0.2682 - val_loss: 0.1202 - val_mae: 0.2469\n",
            "Epoch 116/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2688 - val_loss: 0.1200 - val_mae: 0.2460\n",
            "Epoch 117/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2690 - val_loss: 0.1202 - val_mae: 0.2462\n",
            "Epoch 118/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2688 - val_loss: 0.1200 - val_mae: 0.2459\n",
            "Epoch 119/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1337 - mae: 0.2686 - val_loss: 0.1201 - val_mae: 0.2458\n",
            "Epoch 120/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1337 - mae: 0.2685 - val_loss: 0.1202 - val_mae: 0.2469\n",
            "Epoch 121/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1339 - mae: 0.2682 - val_loss: 0.1201 - val_mae: 0.2460\n",
            "Epoch 122/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2683 - val_loss: 0.1200 - val_mae: 0.2452\n",
            "Epoch 123/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2684 - val_loss: 0.1203 - val_mae: 0.2463\n",
            "Epoch 124/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1336 - mae: 0.2688 - val_loss: 0.1200 - val_mae: 0.2452\n",
            "Epoch 125/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1336 - mae: 0.2681 - val_loss: 0.1202 - val_mae: 0.2452\n",
            "Epoch 126/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1338 - mae: 0.2681 - val_loss: 0.1201 - val_mae: 0.2454\n",
            "Epoch 127/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2678 - val_loss: 0.1205 - val_mae: 0.2475\n",
            "Epoch 128/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2682 - val_loss: 0.1202 - val_mae: 0.2464\n",
            "Epoch 129/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2690 - val_loss: 0.1201 - val_mae: 0.2462\n",
            "Epoch 130/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2685 - val_loss: 0.1203 - val_mae: 0.2461\n",
            "Epoch 131/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2691 - val_loss: 0.1202 - val_mae: 0.2456\n",
            "Epoch 132/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1336 - mae: 0.2684 - val_loss: 0.1201 - val_mae: 0.2448\n",
            "Epoch 133/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2684 - val_loss: 0.1201 - val_mae: 0.2455\n",
            "Epoch 134/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2680 - val_loss: 0.1202 - val_mae: 0.2462\n",
            "Epoch 135/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2683 - val_loss: 0.1202 - val_mae: 0.2461\n",
            "Epoch 136/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1335 - mae: 0.2683 - val_loss: 0.1204 - val_mae: 0.2466\n",
            "Epoch 137/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1333 - mae: 0.2693 - val_loss: 0.1206 - val_mae: 0.2464\n",
            "Epoch 138/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1338 - mae: 0.2681 - val_loss: 0.1205 - val_mae: 0.2463\n",
            "Epoch 139/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1337 - mae: 0.2679 - val_loss: 0.1202 - val_mae: 0.2457\n",
            "Epoch 140/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1337 - mae: 0.2683 - val_loss: 0.1203 - val_mae: 0.2462\n",
            "Epoch 141/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2686 - val_loss: 0.1203 - val_mae: 0.2459\n",
            "Epoch 142/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1336 - mae: 0.2682 - val_loss: 0.1204 - val_mae: 0.2463\n",
            "Epoch 143/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1336 - mae: 0.2686 - val_loss: 0.1202 - val_mae: 0.2456\n",
            "Epoch 144/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2687 - val_loss: 0.1202 - val_mae: 0.2449\n",
            "Epoch 145/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2669 - val_loss: 0.1201 - val_mae: 0.2454\n",
            "Epoch 146/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2683 - val_loss: 0.1203 - val_mae: 0.2462\n",
            "Epoch 147/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2684 - val_loss: 0.1203 - val_mae: 0.2464\n",
            "Epoch 148/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2687 - val_loss: 0.1203 - val_mae: 0.2468\n",
            "Epoch 149/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2681 - val_loss: 0.1205 - val_mae: 0.2465\n",
            "Epoch 150/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1337 - mae: 0.2684 - val_loss: 0.1203 - val_mae: 0.2462\n",
            "Epoch 151/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2684 - val_loss: 0.1202 - val_mae: 0.2452\n",
            "Epoch 152/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2679 - val_loss: 0.1203 - val_mae: 0.2456\n",
            "Epoch 153/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2679 - val_loss: 0.1205 - val_mae: 0.2463\n",
            "Epoch 154/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2682 - val_loss: 0.1207 - val_mae: 0.2470\n",
            "Epoch 155/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2687 - val_loss: 0.1202 - val_mae: 0.2453\n",
            "Epoch 156/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1336 - mae: 0.2674 - val_loss: 0.1204 - val_mae: 0.2462\n",
            "Epoch 157/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2686 - val_loss: 0.1204 - val_mae: 0.2463\n",
            "Epoch 158/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2694 - val_loss: 0.1205 - val_mae: 0.2468\n",
            "Epoch 159/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2689 - val_loss: 0.1207 - val_mae: 0.2470\n",
            "Epoch 160/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2680 - val_loss: 0.1204 - val_mae: 0.2460\n",
            "Epoch 161/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2678 - val_loss: 0.1203 - val_mae: 0.2454\n",
            "Epoch 162/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2682 - val_loss: 0.1201 - val_mae: 0.2446\n",
            "Epoch 163/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1336 - mae: 0.2672 - val_loss: 0.1201 - val_mae: 0.2446\n",
            "Epoch 164/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2675 - val_loss: 0.1201 - val_mae: 0.2441\n",
            "Epoch 165/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2668 - val_loss: 0.1203 - val_mae: 0.2452\n",
            "Epoch 166/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2676 - val_loss: 0.1203 - val_mae: 0.2452\n",
            "Epoch 167/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2685 - val_loss: 0.1204 - val_mae: 0.2455\n",
            "Epoch 168/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2681 - val_loss: 0.1203 - val_mae: 0.2458\n",
            "Epoch 169/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1336 - mae: 0.2682 - val_loss: 0.1203 - val_mae: 0.2453\n",
            "Epoch 170/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1337 - mae: 0.2675 - val_loss: 0.1203 - val_mae: 0.2449\n",
            "Epoch 171/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2678 - val_loss: 0.1202 - val_mae: 0.2449\n",
            "Epoch 172/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2680 - val_loss: 0.1203 - val_mae: 0.2452\n",
            "Epoch 173/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2675 - val_loss: 0.1204 - val_mae: 0.2461\n",
            "Epoch 174/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2689 - val_loss: 0.1205 - val_mae: 0.2460\n",
            "Epoch 175/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2679 - val_loss: 0.1206 - val_mae: 0.2461\n",
            "Epoch 176/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2671 - val_loss: 0.1203 - val_mae: 0.2451\n",
            "Epoch 177/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2674 - val_loss: 0.1204 - val_mae: 0.2456\n",
            "Epoch 178/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2677 - val_loss: 0.1206 - val_mae: 0.2463\n",
            "Epoch 179/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1337 - mae: 0.2674 - val_loss: 0.1204 - val_mae: 0.2457\n",
            "Epoch 180/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2671 - val_loss: 0.1203 - val_mae: 0.2457\n",
            "Epoch 181/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2680 - val_loss: 0.1205 - val_mae: 0.2456\n",
            "Epoch 182/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2681 - val_loss: 0.1204 - val_mae: 0.2453\n",
            "Epoch 183/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2676 - val_loss: 0.1204 - val_mae: 0.2454\n",
            "Epoch 184/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2680 - val_loss: 0.1203 - val_mae: 0.2446\n",
            "Epoch 185/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2675 - val_loss: 0.1203 - val_mae: 0.2447\n",
            "Epoch 186/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2672 - val_loss: 0.1204 - val_mae: 0.2456\n",
            "Epoch 187/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2682 - val_loss: 0.1205 - val_mae: 0.2460\n",
            "Epoch 188/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2674 - val_loss: 0.1206 - val_mae: 0.2469\n",
            "Epoch 189/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2685 - val_loss: 0.1206 - val_mae: 0.2462\n",
            "Epoch 190/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2682 - val_loss: 0.1204 - val_mae: 0.2455\n",
            "Epoch 191/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2678 - val_loss: 0.1205 - val_mae: 0.2458\n",
            "Epoch 192/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1332 - mae: 0.2687 - val_loss: 0.1203 - val_mae: 0.2446\n",
            "Epoch 193/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2672 - val_loss: 0.1203 - val_mae: 0.2447\n",
            "Epoch 194/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2670 - val_loss: 0.1204 - val_mae: 0.2449\n",
            "Epoch 195/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2674 - val_loss: 0.1204 - val_mae: 0.2453\n",
            "Epoch 196/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2682 - val_loss: 0.1204 - val_mae: 0.2445\n",
            "Epoch 197/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2675 - val_loss: 0.1205 - val_mae: 0.2451\n",
            "Epoch 198/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1335 - mae: 0.2669 - val_loss: 0.1205 - val_mae: 0.2453\n",
            "Epoch 199/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2674 - val_loss: 0.1206 - val_mae: 0.2455\n",
            "Epoch 200/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1332 - mae: 0.2678 - val_loss: 0.1204 - val_mae: 0.2450\n",
            "Epoch 201/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2666 - val_loss: 0.1205 - val_mae: 0.2453\n",
            "Epoch 202/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2680 - val_loss: 0.1205 - val_mae: 0.2453\n",
            "Epoch 203/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2671 - val_loss: 0.1205 - val_mae: 0.2455\n",
            "Epoch 204/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2670 - val_loss: 0.1205 - val_mae: 0.2454\n",
            "Epoch 205/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2678 - val_loss: 0.1208 - val_mae: 0.2460\n",
            "Epoch 206/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2673 - val_loss: 0.1207 - val_mae: 0.2461\n",
            "Epoch 207/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2689 - val_loss: 0.1209 - val_mae: 0.2462\n",
            "Epoch 208/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2670 - val_loss: 0.1205 - val_mae: 0.2450\n",
            "Epoch 209/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2673 - val_loss: 0.1206 - val_mae: 0.2454\n",
            "Epoch 210/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2674 - val_loss: 0.1206 - val_mae: 0.2450\n",
            "Epoch 211/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2674 - val_loss: 0.1206 - val_mae: 0.2454\n",
            "Epoch 212/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2670 - val_loss: 0.1205 - val_mae: 0.2452\n",
            "Epoch 213/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2676 - val_loss: 0.1205 - val_mae: 0.2453\n",
            "Epoch 214/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2676 - val_loss: 0.1205 - val_mae: 0.2450\n",
            "Epoch 215/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1335 - mae: 0.2674 - val_loss: 0.1204 - val_mae: 0.2446\n",
            "Epoch 216/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1337 - mae: 0.2668 - val_loss: 0.1204 - val_mae: 0.2441\n",
            "Epoch 217/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2670 - val_loss: 0.1205 - val_mae: 0.2452\n",
            "Epoch 218/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2668 - val_loss: 0.1206 - val_mae: 0.2461\n",
            "Epoch 219/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2679 - val_loss: 0.1205 - val_mae: 0.2451\n",
            "Epoch 220/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2671 - val_loss: 0.1207 - val_mae: 0.2454\n",
            "Epoch 221/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2677 - val_loss: 0.1206 - val_mae: 0.2451\n",
            "Epoch 222/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2677 - val_loss: 0.1206 - val_mae: 0.2458\n",
            "Epoch 223/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2679 - val_loss: 0.1207 - val_mae: 0.2459\n",
            "Epoch 224/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2680 - val_loss: 0.1207 - val_mae: 0.2459\n",
            "Epoch 225/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2676 - val_loss: 0.1206 - val_mae: 0.2450\n",
            "Epoch 226/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2669 - val_loss: 0.1206 - val_mae: 0.2451\n",
            "Epoch 227/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2674 - val_loss: 0.1206 - val_mae: 0.2452\n",
            "Epoch 228/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2672 - val_loss: 0.1206 - val_mae: 0.2451\n",
            "Epoch 229/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1336 - mae: 0.2671 - val_loss: 0.1207 - val_mae: 0.2456\n",
            "Epoch 230/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2680 - val_loss: 0.1208 - val_mae: 0.2463\n",
            "Epoch 231/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2679 - val_loss: 0.1207 - val_mae: 0.2463\n",
            "Epoch 232/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2676 - val_loss: 0.1206 - val_mae: 0.2457\n",
            "Epoch 233/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2677 - val_loss: 0.1207 - val_mae: 0.2460\n",
            "Epoch 234/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2677 - val_loss: 0.1206 - val_mae: 0.2450\n",
            "Epoch 235/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2673 - val_loss: 0.1209 - val_mae: 0.2460\n",
            "Epoch 236/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2672 - val_loss: 0.1209 - val_mae: 0.2461\n",
            "Epoch 237/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2676 - val_loss: 0.1207 - val_mae: 0.2453\n",
            "Epoch 238/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2673 - val_loss: 0.1207 - val_mae: 0.2457\n",
            "Epoch 239/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2678 - val_loss: 0.1210 - val_mae: 0.2457\n",
            "Epoch 240/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2674 - val_loss: 0.1210 - val_mae: 0.2460\n",
            "Epoch 241/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2680 - val_loss: 0.1208 - val_mae: 0.2462\n",
            "Epoch 242/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1334 - mae: 0.2675 - val_loss: 0.1208 - val_mae: 0.2458\n",
            "Epoch 243/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2677 - val_loss: 0.1207 - val_mae: 0.2458\n",
            "Epoch 244/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2681 - val_loss: 0.1208 - val_mae: 0.2462\n",
            "Epoch 245/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2675 - val_loss: 0.1209 - val_mae: 0.2465\n",
            "Epoch 246/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2683 - val_loss: 0.1209 - val_mae: 0.2468\n",
            "Epoch 247/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2679 - val_loss: 0.1209 - val_mae: 0.2462\n",
            "Epoch 248/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2670 - val_loss: 0.1209 - val_mae: 0.2460\n",
            "Epoch 249/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1332 - mae: 0.2676 - val_loss: 0.1209 - val_mae: 0.2467\n",
            "Epoch 250/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2685 - val_loss: 0.1209 - val_mae: 0.2463\n",
            "Epoch 251/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2679 - val_loss: 0.1210 - val_mae: 0.2463\n",
            "Epoch 252/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2679 - val_loss: 0.1209 - val_mae: 0.2465\n",
            "Epoch 253/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2676 - val_loss: 0.1208 - val_mae: 0.2465\n",
            "Epoch 254/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1334 - mae: 0.2676 - val_loss: 0.1208 - val_mae: 0.2459\n",
            "Epoch 255/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1330 - mae: 0.2680 - val_loss: 0.1207 - val_mae: 0.2452\n",
            "Epoch 256/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2673 - val_loss: 0.1208 - val_mae: 0.2456\n",
            "Epoch 257/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1333 - mae: 0.2672 - val_loss: 0.1209 - val_mae: 0.2460\n",
            "Epoch 258/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1333 - mae: 0.2674 - val_loss: 0.1209 - val_mae: 0.2460\n",
            "Epoch 259/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1328 - mae: 0.2674 - val_loss: 0.1207 - val_mae: 0.2454\n",
            "Epoch 260/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2673 - val_loss: 0.1209 - val_mae: 0.2457\n",
            "Epoch 261/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2674 - val_loss: 0.1209 - val_mae: 0.2460\n",
            "Epoch 262/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2669 - val_loss: 0.1208 - val_mae: 0.2459\n",
            "Epoch 263/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2671 - val_loss: 0.1208 - val_mae: 0.2459\n",
            "Epoch 264/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2675 - val_loss: 0.1210 - val_mae: 0.2461\n",
            "Epoch 265/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2677 - val_loss: 0.1211 - val_mae: 0.2465\n",
            "Epoch 266/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2676 - val_loss: 0.1210 - val_mae: 0.2467\n",
            "Epoch 267/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2680 - val_loss: 0.1210 - val_mae: 0.2464\n",
            "Epoch 268/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2675 - val_loss: 0.1211 - val_mae: 0.2467\n",
            "Epoch 269/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2677 - val_loss: 0.1209 - val_mae: 0.2462\n",
            "Epoch 270/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2670 - val_loss: 0.1209 - val_mae: 0.2459\n",
            "Epoch 271/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2673 - val_loss: 0.1209 - val_mae: 0.2448\n",
            "Epoch 272/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2669 - val_loss: 0.1210 - val_mae: 0.2454\n",
            "Epoch 273/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2676 - val_loss: 0.1210 - val_mae: 0.2452\n",
            "Epoch 274/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2673 - val_loss: 0.1210 - val_mae: 0.2452\n",
            "Epoch 275/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2671 - val_loss: 0.1208 - val_mae: 0.2449\n",
            "Epoch 276/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2669 - val_loss: 0.1209 - val_mae: 0.2456\n",
            "Epoch 277/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2673 - val_loss: 0.1210 - val_mae: 0.2459\n",
            "Epoch 278/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2676 - val_loss: 0.1210 - val_mae: 0.2460\n",
            "Epoch 279/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2675 - val_loss: 0.1210 - val_mae: 0.2461\n",
            "Epoch 280/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2683 - val_loss: 0.1210 - val_mae: 0.2461\n",
            "Epoch 281/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2669 - val_loss: 0.1209 - val_mae: 0.2452\n",
            "Epoch 282/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2668 - val_loss: 0.1210 - val_mae: 0.2459\n",
            "Epoch 283/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2675 - val_loss: 0.1212 - val_mae: 0.2463\n",
            "Epoch 284/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2675 - val_loss: 0.1209 - val_mae: 0.2453\n",
            "Epoch 285/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2672 - val_loss: 0.1209 - val_mae: 0.2456\n",
            "Epoch 286/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2673 - val_loss: 0.1209 - val_mae: 0.2453\n",
            "Epoch 287/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2667 - val_loss: 0.1209 - val_mae: 0.2450\n",
            "Epoch 288/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2673 - val_loss: 0.1209 - val_mae: 0.2453\n",
            "Epoch 289/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2671 - val_loss: 0.1210 - val_mae: 0.2459\n",
            "Epoch 290/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2671 - val_loss: 0.1211 - val_mae: 0.2454\n",
            "Epoch 291/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2671 - val_loss: 0.1210 - val_mae: 0.2456\n",
            "Epoch 292/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2668 - val_loss: 0.1209 - val_mae: 0.2454\n",
            "Epoch 293/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1330 - mae: 0.2677 - val_loss: 0.1208 - val_mae: 0.2449\n",
            "Epoch 294/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1333 - mae: 0.2662 - val_loss: 0.1209 - val_mae: 0.2451\n",
            "Epoch 295/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2659 - val_loss: 0.1209 - val_mae: 0.2450\n",
            "Epoch 296/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2672 - val_loss: 0.1209 - val_mae: 0.2448\n",
            "Epoch 297/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2672 - val_loss: 0.1209 - val_mae: 0.2447\n",
            "Epoch 298/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2660 - val_loss: 0.1210 - val_mae: 0.2448\n",
            "Epoch 299/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2664 - val_loss: 0.1211 - val_mae: 0.2461\n",
            "Epoch 300/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2673 - val_loss: 0.1210 - val_mae: 0.2452\n",
            "Epoch 301/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1330 - mae: 0.2673 - val_loss: 0.1212 - val_mae: 0.2453\n",
            "Epoch 302/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1326 - mae: 0.2666 - val_loss: 0.1210 - val_mae: 0.2453\n",
            "Epoch 303/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1329 - mae: 0.2665 - val_loss: 0.1213 - val_mae: 0.2464\n",
            "Epoch 304/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2673 - val_loss: 0.1216 - val_mae: 0.2458\n",
            "Epoch 305/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2672 - val_loss: 0.1212 - val_mae: 0.2453\n",
            "Epoch 306/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2665 - val_loss: 0.1214 - val_mae: 0.2450\n",
            "Epoch 307/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1331 - mae: 0.2669 - val_loss: 0.1211 - val_mae: 0.2445\n",
            "Epoch 308/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2667 - val_loss: 0.1211 - val_mae: 0.2446\n",
            "Epoch 309/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2663 - val_loss: 0.1212 - val_mae: 0.2444\n",
            "Epoch 310/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2660 - val_loss: 0.1210 - val_mae: 0.2449\n",
            "Epoch 311/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2659 - val_loss: 0.1212 - val_mae: 0.2455\n",
            "Epoch 312/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1332 - mae: 0.2663 - val_loss: 0.1212 - val_mae: 0.2459\n",
            "Epoch 313/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2673 - val_loss: 0.1211 - val_mae: 0.2457\n",
            "Epoch 314/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1331 - mae: 0.2676 - val_loss: 0.1212 - val_mae: 0.2466\n",
            "Epoch 315/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1330 - mae: 0.2672 - val_loss: 0.1211 - val_mae: 0.2456\n",
            "Epoch 316/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1333 - mae: 0.2661 - val_loss: 0.1210 - val_mae: 0.2446\n",
            "Epoch 317/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2668 - val_loss: 0.1211 - val_mae: 0.2450\n",
            "Epoch 318/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2663 - val_loss: 0.1211 - val_mae: 0.2447\n",
            "Epoch 319/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2665 - val_loss: 0.1212 - val_mae: 0.2452\n",
            "Epoch 320/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2667 - val_loss: 0.1212 - val_mae: 0.2451\n",
            "Epoch 321/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1328 - mae: 0.2664 - val_loss: 0.1212 - val_mae: 0.2458\n",
            "Epoch 322/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2667 - val_loss: 0.1212 - val_mae: 0.2453\n",
            "Epoch 323/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2669 - val_loss: 0.1212 - val_mae: 0.2451\n",
            "Epoch 324/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2666 - val_loss: 0.1212 - val_mae: 0.2450\n",
            "Epoch 325/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2662 - val_loss: 0.1213 - val_mae: 0.2458\n",
            "Epoch 326/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2671 - val_loss: 0.1213 - val_mae: 0.2453\n",
            "Epoch 327/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1330 - mae: 0.2665 - val_loss: 0.1213 - val_mae: 0.2455\n",
            "Epoch 328/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2656 - val_loss: 0.1211 - val_mae: 0.2450\n",
            "Epoch 329/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2663 - val_loss: 0.1212 - val_mae: 0.2448\n",
            "Epoch 330/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2667 - val_loss: 0.1211 - val_mae: 0.2446\n",
            "Epoch 331/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2659 - val_loss: 0.1213 - val_mae: 0.2454\n",
            "Epoch 332/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2664 - val_loss: 0.1214 - val_mae: 0.2448\n",
            "Epoch 333/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2662 - val_loss: 0.1213 - val_mae: 0.2452\n",
            "Epoch 334/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2667 - val_loss: 0.1213 - val_mae: 0.2457\n",
            "Epoch 335/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1329 - mae: 0.2673 - val_loss: 0.1214 - val_mae: 0.2448\n",
            "Epoch 336/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2664 - val_loss: 0.1213 - val_mae: 0.2450\n",
            "Epoch 337/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1329 - mae: 0.2660 - val_loss: 0.1214 - val_mae: 0.2460\n",
            "Epoch 338/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1326 - mae: 0.2666 - val_loss: 0.1214 - val_mae: 0.2455\n",
            "Epoch 339/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2674 - val_loss: 0.1212 - val_mae: 0.2448\n",
            "Epoch 340/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2657 - val_loss: 0.1211 - val_mae: 0.2439\n",
            "Epoch 341/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1334 - mae: 0.2656 - val_loss: 0.1212 - val_mae: 0.2444\n",
            "Epoch 342/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2665 - val_loss: 0.1212 - val_mae: 0.2446\n",
            "Epoch 343/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2670 - val_loss: 0.1213 - val_mae: 0.2447\n",
            "Epoch 344/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2663 - val_loss: 0.1213 - val_mae: 0.2451\n",
            "Epoch 345/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1327 - mae: 0.2668 - val_loss: 0.1216 - val_mae: 0.2447\n",
            "Epoch 346/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2666 - val_loss: 0.1214 - val_mae: 0.2447\n",
            "Epoch 347/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2664 - val_loss: 0.1213 - val_mae: 0.2450\n",
            "Epoch 348/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2670 - val_loss: 0.1214 - val_mae: 0.2451\n",
            "Epoch 349/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2665 - val_loss: 0.1214 - val_mae: 0.2455\n",
            "Epoch 350/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2665 - val_loss: 0.1214 - val_mae: 0.2454\n",
            "Epoch 351/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2663 - val_loss: 0.1214 - val_mae: 0.2451\n",
            "Epoch 352/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2661 - val_loss: 0.1214 - val_mae: 0.2448\n",
            "Epoch 353/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1329 - mae: 0.2668 - val_loss: 0.1214 - val_mae: 0.2444\n",
            "Epoch 354/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2664 - val_loss: 0.1214 - val_mae: 0.2448\n",
            "Epoch 355/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1330 - mae: 0.2660 - val_loss: 0.1215 - val_mae: 0.2452\n",
            "Epoch 356/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2665 - val_loss: 0.1215 - val_mae: 0.2451\n",
            "Epoch 357/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2659 - val_loss: 0.1215 - val_mae: 0.2446\n",
            "Epoch 358/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2658 - val_loss: 0.1215 - val_mae: 0.2453\n",
            "Epoch 359/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2667 - val_loss: 0.1215 - val_mae: 0.2457\n",
            "Epoch 360/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1329 - mae: 0.2669 - val_loss: 0.1215 - val_mae: 0.2456\n",
            "Epoch 361/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1328 - mae: 0.2669 - val_loss: 0.1214 - val_mae: 0.2451\n",
            "Epoch 362/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1330 - mae: 0.2665 - val_loss: 0.1215 - val_mae: 0.2449\n",
            "Epoch 363/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1333 - mae: 0.2660 - val_loss: 0.1215 - val_mae: 0.2449\n",
            "Epoch 364/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2665 - val_loss: 0.1215 - val_mae: 0.2451\n",
            "Epoch 365/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2664 - val_loss: 0.1216 - val_mae: 0.2456\n",
            "Epoch 366/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1333 - mae: 0.2663 - val_loss: 0.1215 - val_mae: 0.2451\n",
            "Epoch 367/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1329 - mae: 0.2664 - val_loss: 0.1216 - val_mae: 0.2447\n",
            "Epoch 368/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1332 - mae: 0.2660 - val_loss: 0.1215 - val_mae: 0.2443\n",
            "Epoch 369/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1332 - mae: 0.2664 - val_loss: 0.1216 - val_mae: 0.2444\n",
            "Epoch 370/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1331 - mae: 0.2659 - val_loss: 0.1215 - val_mae: 0.2445\n",
            "Epoch 371/400\n",
            "84/84 [==============================] - 0s 4ms/step - loss: 0.1329 - mae: 0.2658 - val_loss: 0.1218 - val_mae: 0.2444\n",
            "Epoch 372/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1335 - mae: 0.2656 - val_loss: 0.1215 - val_mae: 0.2444\n",
            "Epoch 373/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2666 - val_loss: 0.1216 - val_mae: 0.2448\n",
            "Epoch 374/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1327 - mae: 0.2665 - val_loss: 0.1217 - val_mae: 0.2451\n",
            "Epoch 375/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1332 - mae: 0.2659 - val_loss: 0.1218 - val_mae: 0.2453\n",
            "Epoch 376/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2657 - val_loss: 0.1217 - val_mae: 0.2460\n",
            "Epoch 377/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2662 - val_loss: 0.1216 - val_mae: 0.2453\n",
            "Epoch 378/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1327 - mae: 0.2669 - val_loss: 0.1217 - val_mae: 0.2453\n",
            "Epoch 379/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1330 - mae: 0.2665 - val_loss: 0.1216 - val_mae: 0.2452\n",
            "Epoch 380/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1330 - mae: 0.2665 - val_loss: 0.1217 - val_mae: 0.2459\n",
            "Epoch 381/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1329 - mae: 0.2663 - val_loss: 0.1218 - val_mae: 0.2464\n",
            "Epoch 382/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2666 - val_loss: 0.1217 - val_mae: 0.2456\n",
            "Epoch 383/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2658 - val_loss: 0.1216 - val_mae: 0.2446\n",
            "Epoch 384/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1327 - mae: 0.2651 - val_loss: 0.1219 - val_mae: 0.2459\n",
            "Epoch 385/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2664 - val_loss: 0.1218 - val_mae: 0.2456\n",
            "Epoch 386/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1327 - mae: 0.2671 - val_loss: 0.1219 - val_mae: 0.2451\n",
            "Epoch 387/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1331 - mae: 0.2664 - val_loss: 0.1219 - val_mae: 0.2461\n",
            "Epoch 388/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2660 - val_loss: 0.1219 - val_mae: 0.2459\n",
            "Epoch 389/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2664 - val_loss: 0.1219 - val_mae: 0.2460\n",
            "Epoch 390/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1328 - mae: 0.2663 - val_loss: 0.1217 - val_mae: 0.2450\n",
            "Epoch 391/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1328 - mae: 0.2664 - val_loss: 0.1217 - val_mae: 0.2444\n",
            "Epoch 392/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1328 - mae: 0.2660 - val_loss: 0.1216 - val_mae: 0.2443\n",
            "Epoch 393/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1331 - mae: 0.2663 - val_loss: 0.1217 - val_mae: 0.2451\n",
            "Epoch 394/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1330 - mae: 0.2661 - val_loss: 0.1218 - val_mae: 0.2457\n",
            "Epoch 395/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1328 - mae: 0.2666 - val_loss: 0.1219 - val_mae: 0.2456\n",
            "Epoch 396/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1330 - mae: 0.2662 - val_loss: 0.1220 - val_mae: 0.2456\n",
            "Epoch 397/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2660 - val_loss: 0.1218 - val_mae: 0.2450\n",
            "Epoch 398/400\n",
            "84/84 [==============================] - 0s 3ms/step - loss: 0.1332 - mae: 0.2657 - val_loss: 0.1217 - val_mae: 0.2448\n",
            "Epoch 399/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1327 - mae: 0.2657 - val_loss: 0.1219 - val_mae: 0.2462\n",
            "Epoch 400/400\n",
            "84/84 [==============================] - 0s 2ms/step - loss: 0.1329 - mae: 0.2662 - val_loss: 0.1219 - val_mae: 0.2457\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guMjtfa42ahM"
      },
      "source": [
        "### Run with Test Data\n",
        "Put our test data into the model and plot the predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# convert inputs_test from (266,3) to (266,7)\n",
        "padding = NUM_CLASSES-3\n",
        "if padding > 0:\n",
        "  inputs_test_plt = np.hstack((inputs_test, np.zeros((266*padding)).reshape(266,padding)))\n",
        "else:\n",
        "  inputs_test_plt = inputs_test"
      ],
      "metadata": {
        "id": "VP7Ac9pFTRRh"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3Y0CCWJz2EK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b407f0f3-2571-45de-c6f6-9c8f1b411c33"
      },
      "source": [
        "# use the model to predict the test inputs\n",
        "predictions = model.predict(inputs_test)\n",
        "\n",
        "# print the predictions and the expected ouputs\n",
        "print(\"predictions =\\n\", np.round(predictions, decimals=3))\n",
        "print(\"actual =\\n\", outputs_test)\n",
        "\n",
        "# Plot the predictions along with to the test data\n",
        "plt.clf()\n",
        "plt.title('Training data predicted vs actual values')\n",
        "plt.plot(inputs_test_plt, outputs_test, 'b.', label='Actual')\n",
        "plt.plot(inputs_test_plt, predictions, 'r.', label='Predicted')\n",
        "plt.show()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4/4 [==============================] - 0s 3ms/step\n",
            "predictions =\n",
            " [[0.48  0.52  0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.517 0.483 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.005 0.004 0.992]\n",
            " [0.005 0.004 0.991]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.517 0.483 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.004 0.003 0.992]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.47  0.53  0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.004 0.003 0.993]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.485 0.515 0.   ]\n",
            " [0.004 0.003 0.992]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.47  0.53  0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.476 0.524 0.   ]\n",
            " [0.517 0.483 0.   ]\n",
            " [0.004 0.003 0.993]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.47  0.53  0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.48  0.52  0.   ]\n",
            " [0.005 0.004 0.99 ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.004 0.003 0.992]\n",
            " [0.517 0.483 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.005 0.004 0.992]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.004 0.003 0.992]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.517 0.483 0.   ]\n",
            " [0.47  0.53  0.   ]\n",
            " [0.464 0.536 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.517 0.483 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.517 0.483 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.494 0.506 0.   ]\n",
            " [0.485 0.515 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.005 0.004 0.992]\n",
            " [0.005 0.004 0.991]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.004 0.003 0.993]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.463 0.537 0.   ]\n",
            " [0.005 0.004 0.991]\n",
            " [0.005 0.004 0.992]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.47  0.53  0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.464 0.536 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.005 0.004 0.992]\n",
            " [0.011 0.009 0.98 ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.517 0.483 0.   ]\n",
            " [0.005 0.004 0.991]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.005 0.004 0.992]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.514 0.486 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.476 0.524 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.463 0.537 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.498 0.501 0.001]\n",
            " [0.479 0.521 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.005 0.004 0.991]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.503 0.497 0.   ]\n",
            " [0.507 0.493 0.   ]\n",
            " [0.491 0.509 0.   ]\n",
            " [0.496 0.504 0.   ]]\n",
            "actual =\n",
            " [[1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x100 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAy0AAACPCAYAAAD3LDR+AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyJ0lEQVR4nO3deXxM5/4H8M9kIjOpbEIkQYok9i1uiIpdtWgq9iZ0SYRWi0uvLuqqtZbeLnRVKhott0Q1qKWWWn5VS9Pa6aUSoUHtTJASmXl+f0xnZDJnJjOTWU74vF+veY155pzn+T7nPGdyvmejEEIIEBERERERyZSXpwMgIiIiIiKyhkkLERERERHJGpMWIiIiIiKSNSYtREREREQka0xaiIiIiIhI1pi0EBERERGRrDFpISIiIiIiWWPSQkREREREssakhYiIiIiIZI1JCxE5TWpqKurUqePQvFOmTIFCoXBuQOXUuXNndO7c2dNhyI5CocCUKVOMnxctWgSFQoFTp055LKbSSsdI9qlTpw5SU1Pd3i7XGxFZwqSF6AGgUChsem3fvt3Tod4XCgsLMWXKFC7PMnz99df44IMPPB1GhcAxRUQPOm9PB0BErrd48WKTz1999RU2b95sVt6oUaNytbNgwQLodDqH5n3zzTfxxhtvlKt9uSgsLMTUqVMB4IE4U/Pss88iOTkZKpXKrvm+/vprHDlyBC+//LJrAruPPGhjioioNCYtRA+AZ555xuTznj17sHnzZrPy0goLC/HQQw/Z3E6lSpUcig8AvL294e3NnyRXsnd92kqpVEKpVDq9XiIiIgNeHkZEAPRHb5s2bYq9e/eiY8eOeOihh/Dvf/8bALB69WokJCSgRo0aUKlUiIqKwltvvQWtVmtSR+l7Wk6dOgWFQoH33nsPn3/+OaKioqBSqdC6dWv88ssvJvNK3dOiUCgwatQorFq1Ck2bNoVKpUKTJk2wYcMGs/i3b9+OVq1aQa1WIyoqCvPnz7frPhlDfL6+voiLi8OOHTvMpikqKsKkSZMQGxuLwMBAVK5cGR06dMC2bdtM+hwSEgIAmDp1qvHSO8N1+ocOHUJqaioiIyOhVqsRFhaGtLQ0XLlypcwYt2/fDoVCgczMTPz73/9GWFgYKleujMTEROTn55tMa2193rlzB5MnT0Z0dDRUKhUiIiLw+uuv486dOyZ13LlzB//6178QEhICf39/JCYm4syZM2ZxWbqn5fvvv0enTp3g7++PgIAAtG7dGl9//bUxvnXr1uH06dPGZVRy7Dg7xtIuXLgAb29v49mLko4fPw6FQoFPPvkEAHD37l1MnToV9erVg1qtRtWqVdG+fXts3rzZahtXr17Fq6++imbNmsHPzw8BAQHo2bMnDh48aDbt7du3MWXKFNSvXx9qtRrh4eHo168fcnNzyxxTlu69krrH7L333kN8fDyqVq0KX19fxMbGYsWKFWUur9Lu3r2L4OBgDBkyxOy7goICqNVqvPrqqwBs224ssXSfnKVte8mSJYiNjYWvry+Cg4ORnJxstm2cOHEC/fv3R1hYGNRqNWrVqoXk5GRoNBobe09EnsDDmkRkdOXKFfTs2RPJycl45plnEBoaCkC/U+rn54exY8fCz88PW7duxaRJk1BQUIB33323zHq//vpr3LhxA8OHD4dCocA777yDfv364eTJk2Wenfnpp5+QlZWFESNGwN/fHx999BH69++PP/74A1WrVgUA7N+/Hz169EB4eDimTp0KrVaLadOmGXf0yrJw4UIMHz4c8fHxePnll3Hy5EkkJiYiODgYERERxukKCgqQnp6OQYMG4fnnn8eNGzewcOFCdO/eHdnZ2YiJiUFISAg+++wzvPTSS+jbty/69esHAGjevDkAYPPmzTh58iSGDBmCsLAwHD16FJ9//jmOHj2KPXv22JRkzZgxAwqFAuPGjcPFixfxwQcfoFu3bjhw4AB8fX2N00mtT51Oh8TERPz000944YUX0KhRIxw+fBhz5szB77//jlWrVhnnHzZsGJYsWYLBgwcjPj4eW7duRUJCgk3LdNGiRUhLS0OTJk0wfvx4BAUFYf/+/diwYQMGDx6MCRMmQKPR4MyZM5gzZw4AwM/PDwDcEmNoaCg6deqE5cuXY/LkySbfZWZmQqlUYuDAgQD0O8izZs3CsGHDEBcXh4KCAvz666/Yt28fHnvsMYttnDx5EqtWrcLAgQNRt25dXLhwAfPnz0enTp3w22+/oUaNGgAArVaLJ598Elu2bEFycjLGjBmDGzduYPPmzThy5Ai6detmdUzZ48MPP0RiYiKefvppFBUVYdmyZRg4cCDWrl1r87oF9GdV+/bti6ysLMyfPx8+Pj7G71atWoU7d+4gOTkZgG3bjTPMmDEDEydOxFNPPYVhw4bh0qVL+Pjjj9GxY0fs378fQUFBKCoqQvfu3XHnzh3885//RFhYGM6ePYu1a9fi+vXrCAwMdEosROQCgogeOCNHjhSlN/9OnToJAGLevHlm0xcWFpqVDR8+XDz00EPi9u3bxrKUlBRRu3Zt4+e8vDwBQFStWlVcvXrVWL569WoBQKxZs8ZYNnnyZLOYAAgfHx+Rk5NjLDt48KAAID7++GNjWa9evcRDDz0kzp49ayw7ceKE8Pb2NquztKKiIlG9enURExMj7ty5Yyz//PPPBQDRqVMnY1lxcbHJNEIIce3aNREaGirS0tKMZZcuXRIAxOTJk83ak1qWS5cuFQDEjz/+aDXWbdu2CQCiZs2aoqCgwFi+fPlyAUB8+OGHxjJL63Px4sXCy8tL7Nixw6R83rx5AoDYuXOnEEKIAwcOCABixIgRJtMNHjzYrG8ZGRkCgMjLyxNCCHH9+nXh7+8v2rRpI/766y+T+XU6nfHfCQkJJuPFlTFKmT9/vgAgDh8+bFLeuHFj0bVrV+PnFi1aiISEBKt1Sbl9+7bQarUmZXl5eUKlUolp06YZy7744gsBQMyePdusDsPysjamOnXqZDJODUpvj0KYj7+ioiLRtGlTk/4KIUTt2rVFSkqKld4JsXHjRrPtWAghnnjiCREZGWn8bOt2I4Qw66NUH4Qw/704deqUUCqVYsaMGSbTHT58WHh7exvL9+/fLwCIb775xmrfiEh+eHkYERmpVCrJyz1KHr2/ceMGLl++jA4dOqCwsBDHjh0rs96kpCRUqVLF+LlDhw4A9Eeiy9KtWzdERUUZPzdv3hwBAQHGebVaLX744Qf06dPHeOQaAKKjo9GzZ88y6//1119x8eJFvPjiiyZHi1NTU82OuiqVSuM0Op0OV69eRXFxMVq1aoV9+/aV2RZguixv376Ny5cv45FHHgEAm+t47rnn4O/vb/w8YMAAhIeHY/369SbTSa3Pb775Bo0aNULDhg1x+fJl46tr164AYLxkx1DX6NGjTea35ab5zZs348aNG3jjjTegVqtNvrPlTJI7YgSAfv36wdvbG5mZmcayI0eO4LfffkNSUpKxLCgoCEePHsWJEydsqtdApVLBy0v/Z1ar1eLKlSvw8/NDgwYNTNb1t99+i2rVquGf//ynWR3Ofgx4yfF37do1aDQadOjQweaxV1LXrl1RrVo1k+V37do1bN682WT5OWO7KUtWVhZ0Oh2eeuopkzETFhaGevXqGceMYZveuHEjCgsLndI2EbkHkxYiMqpZs6bJjrvB0aNH0bdvXwQGBiIgIAAhISHGm/htuQ784YcfNvlsSGCuXbtm97yG+Q3zXrx4EX/99Reio6PNppMqK+306dMAgHr16pmUV6pUCZGRkWbTf/nll2jevLnx3oaQkBCsW7fO5uvhr169ijFjxiA0NBS+vr4ICQlB3bp1Adi2LKViVSgUiI6ONrunRGp9njhxAkePHkVISIjJq379+gD0yxPQLxcvLy+ThBEAGjRoUGZ8ubm5AICmTZva1J/S3BEjAFSrVg2PPvooli9fbizLzMyEt7e38RIsAJg2bRquX7+O+vXro1mzZnjttddw6NChMuvX6XSYM2cO6tWrB5VKhWrVqiEkJASHDh0yWde5ublo0KCBWx5EsXbtWjzyyCNQq9UIDg42Xs7oyP0c3t7e6N+/P1avXm281ygrKwt37941SVqA8m83ZTlx4gSEEKhXr57ZuPnf//5nHDN169bF2LFjkZ6ejmrVqqF79+749NNPeT8LUQXAe1qIyKjkUViD69evo1OnTggICMC0adMQFRUFtVqNffv2Ydy4cTY94tjSk6WEEC6d19mWLFmC1NRU9OnTB6+99hqqV68OpVKJWbNmGXfUy/LUU09h165deO211xATEwM/Pz/odDr06NHD4cdFWyK1PnU6HZo1a4bZs2dLzlPyHh5PcWeMycnJGDJkCA4cOICYmBgsX74cjz76KKpVq2acpmPHjsjNzcXq1auxadMmpKenY86cOZg3bx6GDRtmse6ZM2di4sSJSEtLw1tvvYXg4GB4eXnh5Zdfduq6VigUkttD6Qdl7NixA4mJiejYsSPmzp2L8PBwVKpUCRkZGcYHJNgrOTkZ8+fPx/fff48+ffpg+fLlaNiwIVq0aGGcpjzbjaUzTaX7ptPpoFAo8P3330v+ZhjulwKA999/H6mpqcb1OXr0aMyaNQt79uxBrVq17Ok+EbkRkxYismr79u24cuUKsrKy0LFjR2N5Xl6eB6O6p3r16lCr1cjJyTH7TqqstNq1awPQH6k1XH4E6J+OlJeXZ7LztWLFCkRGRiIrK8tkZ6r0jdyWdrSuXbuGLVu2YOrUqZg0aZKx3N7LjkpPL4RATk6OTTdmR0VF4eDBg3j00UetXnpUu3Zt6HQ641kAg+PHj9vUBqC/1Mra2S5L7bsjRoM+ffpg+PDhxkucfv/9d4wfP95sOsOTsoYMGYKbN2+iY8eOmDJlitWkZcWKFejSpQsWLlxoUn79+nWTpCgqKgo///wz7t69a/HBFNaWQ5UqVSQvtTScRTT49ttvoVarsXHjRpP/UycjI8Ni3WXp2LEjwsPDkZmZifbt22Pr1q2YMGGCyTS2bjdSqlSpguvXr5uVl+5bVFQUhBCoW7eu8YycNc2aNUOzZs3w5ptvYteuXWjXrh3mzZuH6dOnlzkvEXkGLw8jIqsMRy1LHsktKirC3LlzPRWSCaVSiW7dumHVqlU4d+6csTwnJwfff/99mfO3atUKISEhmDdvHoqKiozlixYtMttZkloWP//8M3bv3m0yneH/QrFlfgB2/6/wX331FW7cuGH8vGLFCvz555823cPz1FNP4ezZs1iwYIHZd3/99Rdu3boFAMa6PvroI7tjffzxx+Hv749Zs2bh9u3bJt+V7HvlypUlL8txR4wGQUFB6N69O5YvX45ly5bBx8cHffr0MZmm9OOo/fz8EB0dbfb45dKUSqXZuv7mm29w9uxZk7L+/fvj8uXLxkcsl2SY39KYAvQ77MeOHcOlS5eMZQcPHsTOnTvN4lEoFCZnKU6dOmXyNDZ7eXl5YcCAAVizZg0WL16M4uJis0vDbN1upERFRUGj0Zhcjvfnn39i5cqVJtP169cPSqUSU6dONVvmQgjjOiwoKEBxcbHJ982aNYOXl1eZ65OIPItnWojIqvj4eFSpUgUpKSkYPXo0FAoFFi9e7JHLsyyZMmUKNm3ahHbt2uGll16CVqvFJ598gqZNm+LAgQNW561UqRKmT5+O4cOHo2vXrkhKSkJeXh4yMjLM7ml58sknkZWVhb59+yIhIQF5eXmYN28eGjdujJs3bxqn8/X1RePGjZGZmYn69esjODgYTZs2RdOmTdGxY0e88847uHv3LmrWrIlNmzbZfdYqODgY7du3x5AhQ3DhwgV88MEHiI6OxvPPP1/mvM8++yyWL1+OF198Edu2bUO7du2g1Wpx7NgxLF++HBs3bkSrVq0QExODQYMGYe7cudBoNIiPj8eWLVtsOnsVEBCAOXPmYNiwYWjdujUGDx6MKlWq4ODBgygsLMSXX34JAIiNjUVmZibGjh2L1q1bw8/PD7169XJLjCUlJSXhmWeewdy5c9G9e3cEBQWZfN+4cWN07twZsbGxCA4Oxq+//ooVK1Zg1KhRVut98sknMW3aNAwZMgTx8fE4fPgw/vvf/5qNq+eeew5fffUVxo4di+zsbHTo0AG3bt3CDz/8gBEjRqB3795Wx1RaWhpmz56N7t27Y+jQobh48SLmzZuHJk2aoKCgwNhOQkICZs+ejR49emDw4MG4ePEiPv30U0RHR9t0j4615ffxxx9j8uTJaNasGRo1amS2HGzZbqQkJydj3Lhx6Nu3L0aPHo3CwkJ89tlnqF+/vslN/FFRUZg+fTrGjx+PU6dOoU+fPvD390deXh5WrlyJF154Aa+++iq2bt2KUaNGYeDAgahfvz6Ki4uxePFiKJVK9O/f3+FlQERu4P4HlhGRp1l65HGTJk0kp9+5c6d45JFHhK+vr6hRo4Z4/fXXjY873bZtm3E6S488fvfdd83qRKlHm1p65PHIkSPN5pV6HOuWLVtEy5YthY+Pj4iKihLp6enilVdeEWq12sJSMDV37lxRt25doVKpRKtWrcSPP/5o9ihZnU4nZs6cKWrXri1UKpVo2bKlWLt2reRjWXft2iViY2OFj4+PSV/PnDkj+vbtK4KCgkRgYKAYOHCgOHfunE2P6DU88njp0qVi/Pjxonr16sLX11ckJCSI06dPm0xrbX0WFRWJ//znP6JJkyZCpVKJKlWqiNjYWDF16lSh0WiM0/31119i9OjRomrVqqJy5cqiV69eIj8/v8xHHht89913Ij4+Xvj6+oqAgAARFxcnli5davz+5s2bYvDgwSIoKEgAMFmGzo7RmoKCAuHr6ysAiCVLlph9P336dBEXFyeCgoKEr6+vaNiwoZgxY4YoKiqyWu/t27fFK6+8IsLDw4Wvr69o166d2L17t+QjigsLC8WECRNE3bp1RaVKlURYWJgYMGCAyM3NNU5jaUwJIcSSJUtEZGSk8PHxETExMWLjxo2S43LhwoWiXr16QqVSiYYNG4qMjAzJbc+WRx4b6HQ6ERERIQCI6dOnS35v63Yjtd42bdokmjZtKnx8fESDBg3EkiVLJGMWQohvv/1WtG/fXlSuXFlUrlxZNGzYUIwcOVIcP35cCCHEyZMnRVpamoiKihJqtVoEBweLLl26iB9++MGmvhKR5yiEkNHhUiIiJ+rTp49Dj6qVq+3bt6NLly745ptvMGDAAE+HQ0RE5Da8p4WI7gt//fWXyecTJ05g/fr16Ny5s2cCIiIiIqfhPS1EdF+IjIxEamoqIiMjcfr0aXz22Wfw8fHB66+/7unQiIiIqJyYtBDRfaFHjx5YunQpzp8/D5VKhbZt22LmzJlm/xEjERERVTy8p4WIiIiIiGSN97QQEREREZGsMWkhIiIiIiJZs/uelh9//BHvvvsu9u7da/xfaUv/78HW6HQ6nDt3Dv7+/lAoFPY2T0RERERE9wkhBG7cuIEaNWrAy8vy+RS7k5Zbt26hRYsWSEtLQ79+/ewO7Ny5c4iIiLB7PiIiIiIiuj/l5+ejVq1aFr+3O2np2bMnevbs6XBA/v7+xsACAgIcrscZzp4FcnOBqCigZk3b55sxA/juOyAxEZgwwbG2//tfYM0aoFcv4OmnHavDoF074OhRoEkTYOdO2+ZxtO8G48YB69cDTzwB/Oc/9s9fkjOXBQBs2ABs2gQ8/jjQo0f565NS3uUnx/b27gV27wbatgViY13ThoG7l59cYyjp0IazyN2Ui6jHo9C8h3RAv87YAO2XS3C3chDUA55EpcCHENo2CgBwYXeu8d85n3wP3Z8XoIyqA+3BQ4AAQkY8hSoNw3D8raXwPvk7VEn90GqCizYQssoZY09u49cR1vpQ0fvnzt9TVxkyBNi6FejaFcjIMP1OTuvHkX2ykSOBjRuB7t2BTz91vO3eve+t59WrHa/H3Zy931VeBQUFiIiIMOYIFolyACBWrlxp1zwajUYAEBqNpjxNl1t6uhBeXkIA+vf0dNvm8/PTz2N4+fnZ33ZUlGkdUVH212FQsh7DqyyO9t1ApTJtT6VyLHYhnLsshBAiPt60vvj48tUnpbzLT47tpaSYLreUFOe3YeDu5SfXGEpaGJ8uiqEQAhDFUIiF8eYBHfSLF7oSK8nwby0UQvv3vFooJKcx/Lv054N+LthAyCpnjD25jV9HWOtDRe+fO39PXcWw/A0vL69738lp/TiyT+btbTqPt7djbTuy/yUHzt7vcgZbcwOXJy23b98WGo3G+MrPz/d40pKfb75BKpX6cmsmTpQepBMn2t52RoZ0HRkZ9vejRQvpulq0sDyPo303GDNGus0xY+yP35nLQggh1qyRrm/NGsfqk1Le5SfH9rKzpZdbdrbz2jBw9/KTawwl7V+Tb0w6DK9iKMT+NfcC2jNxjUnC4ayXDhB7JjpxAyGrnDH25DZ+HWGtDxW9f+78PXWVpCTpPiQlyWv9OLJPlpYmPU9amn1td+smXU+3buXrk6s5e7/LWWxNWlz+9LBZs2YhMDDQ+JLD/SwnTgA6nWmZVgvk5FifLytLunzlStvbtjStI6cVDx+2rxxwvO8GluJ0JH5nLgtAf7malA0bHKtPSnmXnxzb27FDutzWSw3t4e7lJ9cYSjqzfBe8IEzKlBA4u2K38fPtrPVwxWNLFABur3TiBkJWOWPsyW38OsJaHyp6/9z5e+oqmzZZLpfT+nFkn2zdOvvKLfnpJ/vK5cLZ+13u5vKkZfz48dBoNMZXfn6+q5ssU716QOmHEyiVQHS09fksPXegb1/b27Y0be/ettdh0KyZfeWA4303sBSnI/E7c1kA+vtrpDjzvpbyLj85ttehg3R5u3bOa8PA3ctPrjGU1LChdHmDBvf+re73RKm0xjkEAHVf3tfiLs4Ye3Ibv46w1oeK3j93/p66yuOPWy6X0/pxZJ8sIcG+ckvat7evXC6cvd/lduU5nQMbLg8rTU73tCiV905tPmj3tDjSdwPe01K+5SfH9tx9T4s7l59cYzDKN788TAsvs+stLN3TUgwvk/theE+LvDlj7Mlq/DrIWh8qev8ehHta5LJ+eE+L/SryPS0KIYRdB/Bu3ryJnL/PA7Zs2RKzZ89Gly5dEBwcjIcffrjM+QsKChAYGAiNRuPxp4edOaM/pRkdDVh5wpqZSZP0p9j69gWmTXOs7UWL9KfjevcGUlMdq8MgJkZ/SVizZsCBA7bN42jfDV5++V78H3xg//wlOXNZAMDatfpLwnr0AJ58svz1SSnv8pNje7/8or+EoV07oHVr17Rh4O7lJ9cYjBYuhHhhOBQ6LYSXEorP5wNDh5pN9vOktShe+CWKKwdBndwbqmA/hLbTH+K8sDPH+O/f56yF7tx5/dPD9h0CAFQfMwjBTcJxbMJieJ88Dp/BA9Bmmos2ELLKGWNPVuPXQdb6UNH7587fU1dJTr73JM5ly0y/k9P6cWSfbOhQ/SVhCQnAwoWOt/3YY/pLwtq3BzZvdrwed3P2fld52Zob2J20bN++HV26dDErT0lJwaJFi5wWGBHRA0VOewFERERuYmtuYPf/09K5c2fYmecQEVFZatViskJERGSBy2/EJyIiIiIiKg8mLUREREREJGtMWoiIiIiISNaYtBARERERkawxaSEiIiIiIllj0kJERERERLLGpIWIiIiIiGSNSQsREREREckakxYiIiIiIpI1Ji1ERERERCRrTFqIiIiIiEjWmLQQEREREZGsMWkhIiIiIiJZY9JCRERERESyxqSFiIiIiIhkjUkLERERERHJGpMWIiIiIiKSNSYtREREREQka0xaiIiIiIhI1pi0EBERERGRrDFpISIiIiIiWWPSQkREREREssakhYiIiIiIZI1JC5EjfvkFmD1b/36/uB/7RERERPcFJi3keXFxgLe3/t0ZFi0CevfWv9vizBlg2zb9uy1SU/WxvvKK/j011bE47eHqhMLdfWKCRERERHZg0uIp9u4oO7uu8rbvrERDodDvuGq1+neFonz1RUcDQ4YA332nf4+Otj79woVA7dpA167694ULrU//yy/Al1+aln35pWt3vl2dULi7T55I+oiIiAxiYgClUv8uZ87cVyxp6FAgLEz/XoEwaXFEeQe7vTvKZdUVEaGvKyLCtrrK276zEg1LCY+jidCiRUBurmlZbq7lMy5nzgAvvADodPrPOh0wfLj1H4cdO6TLd+60N1rbuCOhcGefPJH0WeKqPwbkHlx/RJ7jru3PlnbsPYiqUAAHD+r/5h88WL6Dpf7++vn9/R2vwxJn7iuWVKkS8MUXwIUL+vdKlZxTrxs82EmLIxtdeQe7IzvK1uoaNsy0bNgw63WVt31nJhr79tlXXpaVK6XLV6+WLj9x4t5yMNBqgZwcy2106CBd3q5d2fE5wh0JRf360uVlnaVyhLuTPktc9ceA3MOV688dO2POuDySl1iSs1ga85aSAXf9ftrSjr0HUS0dbHbkILRCAdy8qf/3zZvlv1KkJGfuK5Y0dChQXGxaVlxccc64CDfTaDQCgNBoNO5u2lR6uhBeXkIA+vf09LLnadFCP33pV4sWtre7dat0Hdu22d8HtVq6LrXade0rldLzK5X2x9+6tXRdrVvbX5cQQmRkSNeXkSE9fX7+vTFQsh/5+dbbSUkxnSclxbF4bZGdLd2n7GznteHMMVkWd/SnLI6ud5IHV64/R/4u2MsZvx/u/A1ypexsId5/X3r7t/ZdRfD++0LEx+vf5czSmJf6nRbCfb+ftrTjyD5E6ToNLy8v++Lz85Oux8/Psf6W5qq/y6Gh0vWGhjolbEfZmhs8mEmLoxudMwZ7UpJ0HUlJ9vdDqp6SPy5SyvuD4+xEw57YbREVZVpXVJT16dPT7yViSqXtOynZ2ULMmeOeP6iu3kFx9068p3e43JmkkfO5av25YztwRtIuh8TfGaz9Dnj6N6K8QkJM4w8J8XRE0iyNeUsHaFu3dt/vpy3tOHIQ1RkHn4VwbP/LHq76PUpLk445Lc05cTuISYs1jm50zhjszsxyHc30Hd1RN3D2Rtq6tT4ORxOf0jIyhOjTx/IZltLy8/XrXs5H2l2dJJV3TNjLnUlfaTzTUrG5av25Y2fs/fel25gzx711eJq1xKuiJ2WW1o8cz7hYGvOWDtAatrOKfKZFCOfsw7j6TIsQrvu77O1tGrO3t3PqLQcmLdaUZ6Mr72B3dpbraDzl3VF3dqJBnlcRkjdncXeSRs7livXHMy3uYy3xquhJWXy8dPzt2nk6MnOOnGkRwn2/n7a04+g+UIsW+r7be4bFGW3bw1V/l9PS9AfLPXyGxYBJS1nKs9GVd7A7O8s1ZPzOzPCJ7ncPUpJ2P3LF+nPHzhjvaeGZFjmxNObL2iF31++nLe148iAq97+cwtbcQCGEEO688b+goACBgYHQaDQICAhwZ9PmzpzRPykqOhqoVcu9bQ8dCqxbByQk8MlFRERy4Y6/C7/8on9iXrt2QOvWnqvDk1JTTR9/npJy7/H01r6rCKpXBy5duvc5JAS4eNFz8ZTF0piPi9M/zfMf/wCysz0XH933bM0NHuykhYiIiDzDWuJV0ZOy2bOBrCygXz9g7FhPR0Mka0xaiIiIiIhI1mzNDR7s/1ySiIiIiIhkj0kLERERERHJGpMWIiIiIiKSNSYtREREREQka0xaiIiIiIhI1pi0EBERERGRrDFpISIiIiIiWWPSQkREREREssakhYiIiIiIZI1JCxERERERyRqTFiIiIiIikjUmLUREREREJGtMWoiIiIiISNaYtBARERERkawxaSEiIiIiIllj0kJERERERLLGpIWIiIiIiGSNSQsREREREckakxYiIiIiIpI1Ji1ERERERCRrTFqIiIiIiEjWmLQQEREREZGsMWkhIiIiIiJZcyhp+fTTT1GnTh2o1Wq0adMG2dnZzo6LiIiIiIgIgANJS2ZmJsaOHYvJkydj3759aNGiBbp3746LFy+6Ij6i+8MvvwCzZ+vfSzpzBti2Tf++di3Qvz8wbJj5dEOHAmFh+ndrdZesz9lcWTeRO9gyhm3ZVomIyP2EneLi4sTIkSONn7VarahRo4aYNWuWTfNrNBoBQGg0Gnublo20NCFCQ/Xvjnr/fSHi4/Xv5dWtmxBqtf7dXSZOFKJJE/17eSUlCVGliv7dGfLzhdi6Vf/uiumFEGJTnTTxJ0LFpjo2DIKUFKEDhAD07ykp+vL0dKHz8jKWG6Yxm87b23R+b2+LdRv/7eUlRHq67R0SQhxXRIu7gDiuiDb/smSsFuo+kpEttiW+L45kZJfZ1hmEiGJAnEGITbGdy84X+97fKs5l27GSyvB/aRlid2ii+L+0DKfVWR57Jq4R25u8JPZMXOPpUCoEu7dbG8aw1W21RPm1Ce8aZzmSkS22dpgotifNFeey803WY8ltouQY3r8mXyx/aavYv8bx8ezI75bcWOtDRe/fmjVCvPSS/l3uLC3rjAwhEhP1755iyzgYM0aIOnX077Zy1j5M69ZCKJX6d1dw1XYgh3Vbkq25gV1Jy507d4RSqRQrV640KX/uuedEYmKiUwOTK29v436lKL3/aKuQENM6Qmzbb5NUsh7Dy9X8/Ezb8/NzvK6/9yGMLy+v8sWWnn6vTlv22+2dXgghbsM0ibgNK4MgO9skGTHuDK1ZI7QKL+kVWHK6Xr2k509Lk6y75EvrpbT5l05bMuEBhLbkQMrPN4u1dN0/Rpnu7P0YleJYWxJ+TEkXxdC3Xwwv8WOKfcmYlDzvKJMY8ryjyl1neRz0izeJ56BfvEfjkTu7t1sbxrC1bVWqfM+Ad0zGvdmBA6lxDggtFEILhXE8L4y3fzw78rslN9b6UNH7Fx9v+nMcL+PN2dKyjooy7UOUB34ibRkHKpVpnCpV2fU6ax/G1ftfrtoO5LBuS3NJ0nL27FkBQOzatcuk/LXXXhNxcXGS89y+fVtoNBrjKz8/v8ImLWlp0oPUnjMu778vXYcjZ1y6dZOuy5VnXCZOlG7TkaMVSUnSdTl6xiU/3zwJUlrZb7d3eiH0Z1ikdmAsnXE5PUZ6hZ98YoTVhMXwKrRSbqnukq+Ly7eVudyOK6Il+2Q443Ixc6vVuo9kSO/sSZ1xOYMQyWktnXE5l51vTFgMr7tQluuMy/+lZUjG4KkzLnsmWtgp5hkXSY5st2WNYSGE2Jco/eN2NDhesrwYMFtvjrzuQmnXGRdH+i831vpQ0fu3Zo30qpbjGRdLy9rSfoo7j8rbMg7GjJGO09oZF2ftw7RuLV2Ps864uGo7yMjw/LqVYmvS4vKnh82aNQuBgYHGV0REhKubdJl16+wrl/Ltt9LlWVn2x/PTT/aVO4OlOFeutL+uTZvsKy/LiROATmdaptUCOTnOmR4Amp1aB0WpMgWApqekB8EOdIAoVSYAfC96QlvGLWUCgI+F73ws1F1SMZTIQbTVNgAgUuRI9ilS6BfECdQzi7Vk3ZdW7pCc//LqnWZtheGS5LRhuCQZ2/kdJ6CE6UryhhYXdlpZSWXwWbdSMoZK61Y7XGd53M5aLxnP7ZUbPBGO7Dmy3ZY1hgFgd1645LyKq1cly5WA2XpzhDe0yNlg+3h2pP9yY60PFb1/69dLl2+Q4eZsaVlb2k9Z7cafSFvGgaV4rMXprH2YffvsK7eXq7YDS/1057otD7uSlmrVqkGpVOLChQsm5RcuXEBYWJjkPOPHj4dGozG+8vPzHY/WwxIS7CuX0r+/dHm/fvbH0769feXOYCnOvn3tr+vxx+0rL0u9eoBXqRGtVALRFvbb7Z0eAA7XSZBMQo7UkR4E9Z9ujUVIMc4jACxCCh4e8SReVHyOYigBALq/v0Op6YpKlRu+K7JQt+E3rhhKvKSYj4i2tSx35m8nFdGSbZxU6BfEw/G1TGItXXdIX+nErFrvdmZtnUeI5LTnESIZW1gH6Z3N0HZlJ2OWFCX0lYzhbkJvh+ssD3W/JyTjUfft4YlwZM+R7basMQwA1VJ7lUqPAR0U+LnF85IHB7Qw3zYdUQwlonvYPp4d6b/cWOtDRe/fE09Il/eQ4eZsaVlb2k/p7cafSFvGgaV4rMXprH2Yf/zDvnJ7uWo7sNRPd67b8lAIIez63W3Tpg3i4uLw8ccfAwB0Oh0efvhhjBo1Cm+88UaZ82s0GgQFBSE/Px8BAQGORe1BwcH6bNdAqQQsHIizKDISuHLl3ueqVYGTJx2LJzDQvEyjcawuW9WoAdy6de9z5crAuXOO1RUUpD85aaBQANevOx7bV18BY8boj1B4eQEffgg895zzpgeAO4HB8IEWChgSCCVUGsuD4MUXgWNL96INfsbPaIOGg2Ixb56+7f+MPova4iROIhJNcRiD8DU0CMRXSDVOpwsMhAIwticAeP29kkvXfR5hiMRJnFZEYtxHNcvsi4G1NgzLyRCrVN27Yl5E27ylxvl31x2E+APzHGqrtF0vfoW4pWPgDR2K4YXsQR8ifp6NHbPgdHAMHtbmGWP4Q1kXta8eKFed5XGkxmNocivbGM/RynFoem6zx+KRO0e227LGMABMifwKb14ZDW8IFEOB6VU/wpSTzyEvKAZ1RJ7xzIoOwJ5BHwN7dhnHPXAviVFI/FtAf5RQ+3epEgLF8MLSuA/x7Gb7xrMj/Zcba32o6P177DGg5P8EERcHbJbp5mxpWcfEAHl596arWxc4cEAesZUUEgIUFd377OMDXJI+cW/krH0YV+9/uWo7kMO6La2goAARERG4fv06AqUW7N/sTloyMzORkpKC+fPnIy4uDh988AGWL1+OY8eOITQ0tMz5z5w5U6EvESMiIiIiIufKz89HrVqWrxDxtrfCpKQkXLp0CZMmTcL58+cRExODDRs22JSwAECNGjWQn58Pf39/KBTOuCKYKjpDhl1Rz76RPHAckTNwHJEzcByRMzwo40gIgRs3bqBGjRpWp7P7TAuRsxUUFCAwMBAajea+3ijJtTiOyBk4jsgZOI7IGTiOTLn86WFERERERETlwaSFiIiIiIhkjUkLeZxKpcLkyZOhUqk8HQpVYBxH5AwcR+QMHEfkDBxHpnhPCxERERERyRrPtBARERERkawxaSEiIiIiIllj0kJERERERLLGpIWIiIiIiGSNSQu5xaeffoo6depArVajTZs2yM7OtjjtggUL0KFDB1SpUgVVqlRBt27drE5PDw57xlFWVhZatWqFoKAgVK5cGTExMVi8eLEboyW5smcclbRs2TIoFAr06dPHtQFShWDPOFq0aBEUCoXJS61WuzFakit7f4+uX7+OkSNHIjw8HCqVCvXr18f69evdFK1nMWkhl8vMzMTYsWMxefJk7Nu3Dy1atED37t1x8eJFyem3b9+OQYMGYdu2bdi9ezciIiLw+OOP4+zZs26OnOTE3nEUHByMCRMmYPfu3Th06BCGDBmCIUOGYOPGjW6OnOTE3nFkcOrUKbz66qvo0KGDmyIlOXNkHAUEBODPP/80vk6fPu3GiEmO7B1HRUVFeOyxx3Dq1CmsWLECx48fx4IFC1CzZk03R+4hgsjF4uLixMiRI42ftVqtqFGjhpg1a5ZN8xcXFwt/f3/x5ZdfuipEqgDKO46EEKJly5bizTffdEV4VEE4Mo6Ki4tFfHy8SE9PFykpKaJ3795uiJTkzN5xlJGRIQIDA90UHVUU9o6jzz77TERGRoqioiJ3hSgrPNNCLlVUVIS9e/eiW7duxjIvLy9069YNu3fvtqmOwsJC3L17F8HBwa4Kk2SuvONICIEtW7bg+PHj6NixoytDJRlzdBxNmzYN1atXx9ChQ90RJsmco+Po5s2bqF27NiIiItC7d28cPXrUHeGSTDkyjr777ju0bdsWI0eORGhoKJo2bYqZM2dCq9W6K2yP8vZ0AHR/u3z5MrRaLUJDQ03KQ0NDcezYMZvqGDduHGrUqGGyYdODxdFxpNFoULNmTdy5cwdKpRJz587FY4895upwSaYcGUc//fQTFi5ciAMHDrghQqoIHBlHDRo0wBdffIHmzZtDo9HgvffeQ3x8PI4ePYpatWq5I2ySGUfG0cmTJ7F161Y8/fTTWL9+PXJycjBixAjcvXsXkydPdkfYHsWkhWTt7bffxrJly7B9+3betEh28/f3x4EDB3Dz5k1s2bIFY8eORWRkJDp37uzp0KgCuHHjBp599lksWLAA1apV83Q4VIG1bdsWbdu2NX6Oj49Ho0aNMH/+fLz11lsejIwqEp1Oh+rVq+Pzzz+HUqlEbGwszp49i3fffZdJC1F5VatWDUqlEhcuXDApv3DhAsLCwqzO+9577+Htt9/GDz/8gObNm7syTJI5R8eRl5cXoqOjAQAxMTH43//+h1mzZjFpeUDZO45yc3Nx6tQp9OrVy1im0+kAAN7e3jh+/DiioqJcGzTJTnn+rhlUqlQJLVu2RE5OjitCpArAkXEUHh6OSpUqQalUGssaNWqE8+fPo6ioCD4+Pi6N2dN4Twu5lI+PD2JjY7FlyxZjmU6nw5YtW0yOOpX2zjvv4K233sKGDRvQqlUrd4RKMuboOCpNp9Phzp07rgiRKgB7x1HDhg1x+PBhHDhwwPhKTExEly5dcODAAURERLgzfJIJZ/weabVaHD58GOHh4a4Kk2TOkXHUrl075OTkGA+eAMDvv/+O8PDw+z5hAcCnh5HrLVu2TKhUKrFo0SLx22+/iRdeeEEEBQWJ8+fPCyGEePbZZ8Ubb7xhnP7tt98WPj4+YsWKFeLPP/80vm7cuOGpLpAM2DuOZs6cKTZt2iRyc3PFb7/9Jt577z3h7e0tFixY4KkukAzYO45K49PDSAj7x9HUqVPFxo0bRW5urti7d69ITk4WarVaHD161FNdIBmwdxz98ccfwt/fX4waNUocP35crF27VlSvXl1Mnz7dU11wK14eRi6XlJSES5cuYdKkSTh//jxiYmKwYcMG481nf/zxB7y87p30++yzz1BUVIQBAwaY1DN58mRMmTLFnaGTjNg7jm7duoURI0bgzJkz8PX1RcOGDbFkyRIkJSV5qgskA/aOIyIp9o6ja9eu4fnnn8f58+dRpUoVxMbGYteuXWjcuLGnukAyYO84ioiIwMaNG/Gvf/0LzZs3R82aNTFmzBiMGzfOU11wK4UQQng6CCIiIiIiIkt4OImIiIiIiGSNSQsREREREckakxYiIiIiIpI1Ji1ERERERCRrTFqIiIiIiEjWmLQQEREREZGsMWkhIiIiIiJZY9JCRERERESyxqSFiIiIiIhkjUkLERERERHJGpMWIiIiIiKSNSYtREREREQka/8PsBS0lIZKK+YAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7DO6xxXVCym"
      },
      "source": [
        "# Convert the Trained Model to Tensor Flow Lite\n",
        "\n",
        "The next cell converts the model to TFlite format. The size in bytes of the model is also printed out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xn1-Rn9Cp_8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1cd0bb-d385-4154-852d-45575f5ff9d2"
      },
      "source": [
        "# Convert the model to the TensorFlow Lite format without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the model to disk\n",
        "open(\"gesture_model.tflite\", \"wb\").write(tflite_model)\n",
        "\n",
        "import os\n",
        "basic_model_size = os.path.getsize(\"gesture_model.tflite\")\n",
        "print(\"Model is %d bytes\" % basic_model_size)\n",
        "\n",
        ""
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model is 2504 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykccQn7SXrUX"
      },
      "source": [
        "## Encode the Model in an Arduino Header File\n",
        "\n",
        "The next cell creates a constant byte array that contains the TFlite model. Import it as a tab with the sketch below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J33uwpNtAku",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e28b8031-f9f4-43fa-db0e-93af20c61335"
      },
      "source": [
        "!echo \"const unsigned char model[] = {\" > /content/model.h\n",
        "!cat gesture_model.tflite | xxd -i      >> /content/model.h\n",
        "!echo \"};\"                              >> /content/model.h\n",
        "\n",
        "import os\n",
        "model_h_size = os.path.getsize(\"model.h\")\n",
        "print(f\"Header file, model.h, is {model_h_size:,} bytes.\")\n",
        "print(\"\\nOpen the side panel (refresh if needed). Double click model.h to download the file.\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Header file, model.h, is 15,476 bytes.\n",
            "\n",
            "Open the side panel (refresh if needed). Double click model.h to download the file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eSkHZaLzMId"
      },
      "source": [
        "# Realtime Classification of Sensor Data on Arduino\n",
        "\n",
        "Now it's time to switch back to the tutorial instructions and run our new model on the [Arduino Nano 33 BLE Sense](https://www.arduino.cc/en/Guide/NANO33BLE)"
      ]
    }
  ]
}